---
title: "Analisi di Clustering sul Dataset Banknote"
author: "Nome Cognome"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduzione

## Obiettivo del Progetto
Il presente progetto si propone di applicare diverse tecniche di clustering al dataset **banknote** al fine di identificare gruppi omogenei presenti nei dati e confrontare l'efficacia dei vari metodi. L'analisi intende mettere in pratica le metodologie apprese nel modulo di Data Science, offrendo una panoramica completa e comparativa delle tecniche di clustering.

## Contesto e Motivazione
Questo lavoro rientra in un modulo dedicato al clustering, che ha trattato argomenti quali:
- **Clustering Gerarchico Agglomerativo**
- **Clustering Partizionale (K-means e K-medoids)**
- **Clustering Model-based (Mixture of Gaussian, modelli parsimoniosi e mixture of regressions)**
- **Validazione del Clustering**

Il dataset **banknote** rappresenta un caso di studio interessante per valutare come ciascun approccio possa evidenziare strutture nascoste nei dati e per esaminare criticamente i risultati ottenuti.

## Struttura del Documento
Il documento è organizzato nei seguenti capitoli:

1. **Introduzione**: Presentazione del progetto, degli obiettivi e del contesto.
2. **Descrizione del Dataset e Analisi Esplorativa**: Esplorazione preliminare dei dati, analisi descrittiva e operazioni di preprocessing.
3. **Applicazione delle Tecniche di Clustering**: Implementazione dei metodi di clustering (gerarchico, partizionale e model-based).
4. **Validazione e Visualizzazione dei Risultati**: Confronto dei risultati mediante metriche di validazione e visualizzazione dei cluster (con l'ausilio della PCA).
5. **Conclusioni e Sviluppi Futuri**: Sintesi dei risultati ottenuti e prospettive per ulteriori analisi.
6. **Appendici**: Codice completo, grafici supplementari e riferimenti bibliografici.

In questo documento verranno inclusi i blocchi di codice R, commentati e spiegati dettagliatamente, per permettere una replicazione completa dell'analisi e una comprensione approfondita delle tecniche applicate.

# Descrizione del Dataset e Analisi Esplorativa

In questa sezione esamineremo il dataset **banknote** per comprenderne la struttura, le caratteristiche e le relazioni tra le variabili. Il dataset proviene dal pacchetto `mclust` e contiene misure che possono essere utilizzate per distinguere tra banconote autentiche e false (o per altri scopi diagnostici).

## Caricamento del Dataset

```{r load-data, message=FALSE, warning=FALSE}
# Carichiamo il pacchetto mclust e il dataset banknote
library(mclust)
data(banknote)

# Visualizziamo le prime righe del dataset
head(banknote)

#Esplorazione della Struttura e Statistiche Descrittive
# Esaminiamo la struttura del dataset
str(banknote)

# Visualizziamo un riepilogo statistico delle variabili
summary(banknote)

```


## Analisi Esplorativa dei Dati (EDA)

### Matrice di Scatterplot

Per avere una prima idea delle relazioni tra le variabili, creiamo una matrice di scatterplot. Se nel dataset è presente una variabile di classe (ad esempio class), possiamo utilizzarla per colorare i punti.

```{r}
# Se esiste una colonna "class", usiamola per colorare i dati
if("Status" %in% names(banknote)){
  pairs(banknote, col = banknote$Status, 
        main = "Scatterplot Matrix del Dataset Banknote")
} else {
  pairs(banknote, main = "Scatterplot Matrix del Dataset Banknote")
}

```
TODO: Commento:

    Analizza eventuali pattern o raggruppamenti evidenti nei grafici.

    Commenta su come alcune coppie di variabili sembrano essere correlate o meno.

    Se esiste la variabile di classe, valuta se le osservazioni appartengono a gruppi distinti già visibili.

### Analisi delle Correlazioni

Verifichiamo le correlazioni tra le variabili numeriche per capire se esistono relazioni forti che potrebbero influenzare l'analisi di clustering.

```{r}
# Supponendo che le prime 4 colonne siano le variabili quantitative
cor_matrix <- cor(banknote[, 2:7])
print(cor_matrix)
library(reshape2)
library(ggplot2)

cor_melted <- melt(cor_matrix)
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) + geom_tile() +
geom_text(aes(label = round(value, 2)), color = "black",
size = 5) + scale_fill_gradient2(low = "blue", mid = "white",
high = "red", midpoint = 0, limits = c(-1, 1)) + theme_minimal() +
labs(title = "Correlazione variabili", fill = "Correlazione") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

TODO: Commento:

    Osserva i valori di correlazione per identificare variabili fortemente correlate (positivamente o negativamente).

    Discuti l’impatto di eventuali correlazioni elevate: potrebbero ridondare in informazioni e influenzare il clustering.

    Se trovi correlazioni elevate, suggerisci che una riduzione della dimensionalità (es. tramite PCA) potrebbe aiutare.

### Distribuzione delle Variabili

Infine, esaminiamo la distribuzione di ciascuna variabile con degli istogrammi:

```{r}
# Impostiamo un layout per visualizzare gli istogrammi
par(mfrow = c(2, 2))
for(i in 2:7){
  hist(banknote[[i]], main = paste("Istogramma di", names(banknote)[i]),
       xlab = names(banknote)[i], col = "lightblue", border = "white")
}
par(mfrow = c(1, 1))  # Ripristiniamo il layout originale

```
TODO:
Commento:

    Descrivi la forma delle distribuzioni: simmetriche, asimmetriche (skewed), unimodali o multimodali.

    Nota eventuali outlier o code lunghe che potrebbero influenzare la successiva analisi di clustering.

    Confronta le distribuzioni tra le variabili per capire se alcune variabili hanno una variabilità molto diversa, che potrebbe richiedere una normalizzazione/scaling.
    
    

Questa sezione fornisce una panoramica iniziale del dataset banknote, evidenziando la struttura, le principali statistiche descrittive e le relazioni tra le variabili. Queste informazioni sono fondamentali per orientare le successive fasi di clustering e per capire eventuali necessità di preprocessing (ad es. scaling o trasformazioni) prima dell'applicazione dei metodi di clustering.


## 3.1 Clustering Gerarchico Agglomerativo

Il **Clustering Gerarchico Agglomerativo (HAC)** è una tecnica di clustering che costruisce una gerarchia di cluster attraverso un approccio "bottom-up". Inizia considerando ogni punto dati come un cluster separato e, successivamente, unisce iterativamente i cluster più simili fino a formare un unico cluster che racchiude tutti i dati. Questo metodo non richiede la specifica del numero di cluster a priori e produce una rappresentazione ad albero, nota come **dendrogramma**, che facilita la visualizzazione delle relazioni tra i cluster.

### Implementazione in R

Per applicare il clustering gerarchico agglomerativo al nostro dataset, seguiamo questi passaggi:

1. **Calcolo della Matrice di Distanza**: Determiniamo le distanze tra tutte le coppie di punti nel dataset. Una misura comune è la distanza euclidea.

2. **Costruzione del Dendrogramma**: Utilizziamo la funzione `hclust()` per eseguire il clustering gerarchico e generare il dendrogramma.

3. **Taglio del Dendrogramma**: Decidiamo il numero ottimale di cluster tagliando il dendrogramma a un livello appropriato.

```{r hierarchical-clustering}

# 1. Calcolo della Matrice di Distanza
# Utilizziamo la distanza euclidea tra le prime 4 variabili numeriche
distanze <- dist(banknote[, 2:7], method = "euclidean")

# 2. Costruzione del Dendrogramma
# Applichiamo il clustering gerarchico agglomerativo
clustering_hierarchico <- hclust(distanze, method = "ward.D2")

# Visualizziamo il dendrogramma
plot(clustering_hierarchico, main = "Dendrogramma del Clustering Gerarchico Agglomerativo",
     xlab = "", sub = "", cex = 0.9)
```

### Interpretazione del Dendrogramma

Il dendrogramma risultante mostra come i punti dati vengono uniti in cluster successivamente. L'altezza dei rami indica la distanza a cui avviene la fusione: rami più bassi corrispondono a fusioni tra punti o cluster molto simili. Analizzando il dendrogramma, possiamo decidere il numero di cluster tagliando l'albero a un'altezza che separa i gruppi in modo significativo.
Vantaggi e Svantaggi del Clustering Gerarchico

#### Vantaggi:

    Non richiede la specifica del numero di cluster a priori.

    Produce una rappresentazione gerarchica che può essere utile per comprendere le relazioni tra i dati.

    Adatto a dataset di piccole e medie dimensioni.

#### Svantaggi:

    Sensibile alla scelta della misura di distanza e del metodo di linkage.

    Può essere computazionalmente costoso per dataset di grandi dimensioni.

    Una volta effettuata una fusione, non è possibile separare i cluster, rendendo l'algoritmo meno flessibile in caso di errori.
