---
title: "Analisi di Clustering sul Dataset Banknote"
author: "D'Atri, Lombardi, Maiello"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

packages <- c("mclust", "reshape2", "ggplot2", "GGally", "dplyr", "clustertend", "cluster", "FNN", "FactoMineR", "factoextra")

for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        install.packages(pkg, repos = "https://cran.rstudio.com/")
    }
}
```

# 1. Introduzione

## Obiettivo del Progetto

Il presente progetto si propone di applicare diverse tecniche di clustering al dataset **banknote** al fine di identificare gruppi omogenei presenti nei dati e confrontare l'efficacia dei vari metodi. L'analisi intende mettere in pratica le metodologie apprese nel modulo di *Data Analysis & Statistical Learning*, in particolare, verranno trattati i seguenti algoritmi di clustering: **Clustering Gerarchico Agglomerativo**, **Clustering Partizionale (K-means e K-medoids)** e **Clustering Model-based (Mixture of Gaussian)**.

TODO: specificare PCA e mixture of regression

Tra diversi dataset consigliati dai docenti, è stato scelto il dataset **banknote**, per un motivo puramente professionale, essendo tutti i membri del team legati professionalmente alle banconote.  
TODO: togliere?  

Il documento è organizzato nei seguenti capitoli:

1.  **Introduzione**: Presentazione del progetto, degli obiettivi e del contesto.  
2.  **Descrizione del Dataset e Analisi Esplorativa**: Esplorazione preliminare dei dati, analisi descrittiva e operazioni di preprocessing.  
3.  **Applicazione delle Tecniche di Clustering**: Implementazione dei metodi di clustering (gerarchico, partizionale e model-based).  
4.  **Validazione e Visualizzazione dei Risultati**: Confronto dei risultati mediante metriche di validazione e visualizzazione dei cluster (con l'ausilio della PCA).  
5.  **Conclusioni e Sviluppi Futuri**: Sintesi dei risultati ottenuti e prospettive per ulteriori analisi.  


# 2. Descrizione del Dataset e Analisi Esplorativa

In questa sezione esamineremo il dataset **banknote** per comprenderne la struttura, le caratteristiche e le relazioni tra le variabili. Il dataset proviene dal pacchetto `mclust` e contiene misure che possono essere utilizzate per distinguere tra banconote autentiche e false (o per altri scopi diagnostici).
Il dataset contiene le seguenti variabili:

- Status: lo stato della banconota (autentica o contraffatta)

- Length: lunghezza della banconota (mm)

- Left: larghezza del bordo sinistro (mm)

- Right: larghezza del bordo destro (mm)

- Bottom: larghezza del margine inferiore (mm)

- Top: larghezza del margine superiore (mm)

- Diagonal: lunghezza della diagonale (mm)

## Caricamento del Dataset

```{r load-data}
library(mclust)
data(banknote)
```

## Esplorazione della Struttura e Statistiche Descrittive del dataset
Visualizziamo le prime righe del dataset

```{r head}
head(banknote)
```

Esaminiamo la struttura del dataset
```{r str}
str(banknote)
```
Il dataset è composto da 200 osservazioni e 7 variabili, tutte di tipo numerico ad eccezione di 'Status' che è una variabile binaria che può assumere uno tra i seguenti valori: 'counterfeit' e 'genuine'.

Visualizziamo un riepilogo statistico delle variabili.

```{r summary}
summary(banknote)
```
Notiamo che la colonna 'Status' è perfettamente bilanciata.

# Controllo valori mancanti
```{r missing values}
colSums(is.na(banknote))
```
Non c'è nessun valore mancante nelle colonne.

## Analisi Esplorativa dei Dati (EDA)

### Analisi delle Correlazioni

Verifichiamo le correlazioni tra le variabili numeriche per capire se
esistono relazioni forti che potrebbero influenzare l'analisi di
clustering.

```{r}
cor_matrix <- cor(banknote[, -1])
print(cor_matrix)
library(reshape2)
library(ggplot2)

cor_melted <- melt(cor_matrix)
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) + geom_tile() +
geom_text(aes(label = round(value, 2)), color = "black",
size = 5) + scale_fill_gradient2(low = "blue", mid = "white",
high = "red", midpoint = 0, limits = c(-1, 1)) + theme_minimal() +
labs(title = "Correlazione variabili", fill = "Correlazione") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

TODO: Commento:

```
Osserva i valori di correlazione per identificare variabili fortemente correlate (positivamente o negativamente).

Discuti l’impatto di eventuali correlazioni elevate: potrebbero ridondare in informazioni e influenzare il clustering.

Se trovi correlazioni elevate, suggerisci che una riduzione della dimensionalità (es. tramite PCA) potrebbe aiutare.
```

### Distribuzione delle Variabili

Infine, esaminiamo la distribuzione di ciascuna variabile con degli
istogrammi:

```{r}
# Impostiamo un layout per visualizzare gli istogrammi
par(mfrow = c(2, 2))
for(i in 2:7){
    hist(banknote[[i]], main = paste("Istogramma di", names(banknote)[i]),
    xlab = names(banknote)[i], col = "lightblue", border = "white")
}
par(mfrow = c(1, 1))  # Ripristiniamo il layout originale

```

TODO: Commento:

```
Descrivi la forma delle distribuzioni: simmetriche, asimmetriche (skewed), unimodali o multimodali.

Nota eventuali outlier o code lunghe che potrebbero influenzare la successiva analisi di clustering.

Confronta le distribuzioni tra le variabili per capire se alcune variabili hanno una variabilità molto diversa, che potrebbe richiedere una normalizzazione/scaling.
```

### Matrice di Scatterplot

Per avere una prima idea delle relazioni tra le variabili, creiamo una matrice di scatterplot. Utilizziamo la colonna 'Status' per colorare i punti.

```{r scatterplot}
pairs(banknote, col = banknote$Status, main = "Scatterplot Matrix del Dataset Banknote")
# library(GGally)
# library(dplyr)

# ggpairs(banknote, aes(color = as.factor(Status)))

# ggpairs(
#   banknote,
#   aes(color = as.factor(Status))
# ) + theme(plot.margin = margin(0, 0, 0, 0))
```

TODO: Commento:

```
Analizza eventuali pattern o raggruppamenti evidenti nei grafici.

Commenta su come alcune coppie di variabili sembrano essere correlate o meno.

Se esiste la variabile di classe, valuta se le osservazioni appartengono a gruppi distinti già visibili.
```


Questa sezione fornisce una panoramica iniziale del dataset banknote,
evidenziando la struttura, le principali statistiche descrittive e le
relazioni tra le variabili. Queste informazioni sono fondamentali per
orientare le successive fasi di clustering e per capire eventuali
necessità di preprocessing (ad es. scaling o trasformazioni) prima
dell'applicazione dei metodi di clustering.

## Metodi per valutare la tendenza del dataset ad essere clusterizzato
Misure che aiutano a determinare se un dataset è adatto per il clustering, valutando quanto i dati siano distribuiti in modo casuale o contengano una struttura raggruppata.

### Hopkins statistics
La Hopkins Statistics è una misura che può assumere valori tra 0 e 1, più il valore è vicino allo 0 più il dataset ha una struttura clusterizzata (ossia presenta una buona separazione tra gruppi). Se assume un valore tra 0.5 o più significa che il dataset non presenta una struttura di gruppo significativa e probabilmente sono distribuiti con una distribuzione uniforme.
```{r}
library(clustertend)

banknote_num <- banknote[, -1]

set.seed(987)

hopkins_stat <- hopkins(banknote_num, n = nrow(banknote_num) - 1)

print(hopkins_stat)
print(hopkins_stat$H)

```

### VAT
produce una matrice di dissimilarità ordinata, va visto se si vanno a formare lungo la diagonale secondaria del grafico, dei blocchi rossi che rappresentano regioni di osservazioni che condividono una forte similarità

```{r}
#fviz_dist(dist(banknote), show_labels = FALSE) +
```

## PCA
In questa sezione applichiamo la PCA sul dataset, considerando solo le colonne numeriche. Anche se le variabili sono tutte misurate in millimetri e quindi nella stessa scala, applichiamo comunque la standardizzazione per evitare che differenze nella dispersione (variabili con varianze maggiori) influenzino in modo sproporzionato i risultati della PCA.
```{r}
library("FactoMineR")
library("factoextra")

banknote_num <- banknote[, -1]
head(banknote_num)
pca_result <- PCA(banknote_num, graph = FALSE)
```

#### Visualizzazione e interpretazione dei risultati
Estraiamo gli autovalori e calcoliamo sia la varianza spiegata per ciascuna componente che la varianza cumulativa. Queste informazioni ci serviranno per decidere il numero ottimale di componenti da mantenere.
```{r}
eigenvalues <- get_eigenvalue(pca_result)[,1]
explained_variance <- eigenvalues / sum(eigenvalues)
cumulative_variance <- cumsum(explained_variance)

print(eigenvalues)
print(explained_variance)
print(cumulative_variance)
```
Abbiamo ora a disposizione gli autovalori e le informazioni sulla varianza, utili per valutare l'importanza di ciascuna componente principale.

Utilizziamo la Kaiser Rule per selezionare le componenti con autovalori superiori a 1.
```{r kaiser rule}
kaiser_components <- sum(eigenvalues > 1)
cat("Numero di componenti selezionati con la Kaiser rule:", kaiser_components, "\n")
```
Un ulteriore criterio è quello di selezionare il numero minimo di componenti che, sommando la varianza spiegata, raggiungono almeno l'80% del totale. In questo modo garantiamo una buona rappresentazione dei dati.
```{r threshold}
variance_threshold <- min(which(cumulative_variance >= 0.80))

cat("Numero di componenti principali che spiegano l'80% della varianza comulativa:", variance_threshold, "\n")

fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))
```
Nel grafico vediamo la varianza spiegata da ciascuna componente. Con le prime tre componenti principali riusciamo a spiegare l'84.9% della varianza cumulativa.

Vediamo infine un ultimo metodo per scegliere il numero di componenti principali: il metodo dell'elbow.
Mediante un grafico scree plot visualizziamo gli autovalori per ciascuna componente e individuiamo "il gomito", ovvero il punto nel grafico in cui la diminuzione degli autovalori rallenta. Tale punto può essere interpretato come un'indicazione del numero ottimale di componenti. Includiamo nel grafico anche due linee verticali per evidenziare i criteri della varianza cumulativa (rosso) e della Kaiser Rule (blu).
```{r elbow}
scree_plot <- data.frame(PC = 1:length(eigenvalues), Eigenvalue = eigenvalues)

ggplot(scree_plot, aes(x = PC, y = Eigenvalue)) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = variance_threshold, linetype = "dashed", color = "red") +
    geom_vline(xintercept = kaiser_components, linetype = "dotted", color = "blue") +
    labs(title = "Scree Plot for PCA", x = "Principal Component", y = "Eigenvalue") +
    theme_minimal()
```

Nel grafico il gomito sembra collocarsi intorno alla quarta componente. I tre criteri danno perciò risultati differenti, per mantenere un equilibrio tra interpretabilità e preservazione della varianza, si potrebbe optare per 3 componenti.

Per comprendere meglio il contributo delle variabili alle componenti principali, visualizziamo le loro coordinate e realizziamo una mappa di correlazione. Questo ci aiuta a capire come le variabili si relazionano alle componenti principali.
```{r}
var <- get_pca_var(pca_result)

var$coord

fviz_pca_var(pca_result, col.var = "contrib",
            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```
Questo passaggio ci offre una visione immediata di come ogni variabile contribuisce alle componenti principali e della loro interrelazione.

Approfondiamo l'analisi mostrando i contributi delle variabili attraverso diversi metodi grafici, in modo da identificare quali variabili influenzano maggiormente ciascuna componente.
```{r}
print(round(var$contrib,digits = 3))

library("corrplot")
corrplot(var$contrib, is.corr=FALSE)
```
L'analisi dei contributi evidenzia il peso relativo di ciascuna variabile nelle componenti principali, utile per interpretare i risultati della PCA.

Visualizziamo i contributi delle variabili specificamente per la prima componente principale.
```{r}
fviz_contrib(pca_result, choice = "var", axes = 1)
```
Similmente al passaggio precedente, visualizziamo i contributi delle variabili per la seconda componente principale.
```{r}
fviz_contrib(pca_result, choice = "var", axes = 2)
```

# 3. Applicazione delle Tecniche di Clustering

## 3.1 Clustering Gerarchico Agglomerativo

Il **Clustering Gerarchico Agglomerativo (HAC)** è una tecnica di clustering che costruisce una gerarchia di cluster attraverso un approccio "bottom-up". Inizia considerando ogni punto dati come un cluster separato e, successivamente, unisce iterativamente i cluster più
simili fino a formare un unico cluster che racchiude tutti i dati. Questo metodo non richiede la specifica del numero di cluster a priori e produce una rappresentazione ad albero, nota come **dendrogramma**, che facilita la visualizzazione delle relazioni tra i cluster.

### Implementazione in R

Per applicare il clustering gerarchico agglomerativo al nostro dataset,
seguiamo questi passaggi:

1.  **Calcolo della Matrice di Distanza**: Determiniamo le distanze tra
    tutte le coppie di punti nel dataset. Una misura comune è la
    distanza euclidea.

2.  **Costruzione del Dendrogramma**: Utilizziamo la funzione `hclust()`
    per eseguire il clustering gerarchico e generare il dendrogramma.

3.  **Taglio del Dendrogramma**: Decidiamo il numero ottimale di cluster
    tagliando il dendrogramma a un livello appropriato.

```{r hierarchical-clustering}

# # 1. Calcolo della Matrice di Distanza
# # Utilizziamo la distanza euclidea tra le prime 4 variabili numeriche
# distanze <- dist(banknote[, 2:7], method = "euclidean")

# # 2. Costruzione del Dendrogramma
# # Applichiamo il clustering gerarchico agglomerativo
# clustering_hierarchico <- hclust(distanze, method = "ward.D2")

# # Visualizziamo il dendrogramma
# plot(clustering_hierarchico, main = "Dendrogramma del Clustering Gerarchico Agglomerativo",
#      xlab = "", sub = "", cex = 0.9)

# res.agnes <- agnes(x = banknote,
# stand = )
# fviz_dend(res.agnes, cex = 0.6, k = 4)

```

### Interpretazione del Dendrogramma

Il dendrogramma risultante mostra come i punti dati vengono uniti in cluster successivamente. L'altezza dei rami indica la distanza a cui avviene la fusione: rami più bassi corrispondono a fusioni tra punti o cluster molto simili. Analizzando il dendrogramma, possiamo decidere il
numero di cluster tagliando l'albero a un'altezza che separa i gruppi in modo significativo. Vantaggi e Svantaggi del Clustering Gerarchico

#### Vantaggi:

```
Non richiede la specifica del numero di cluster a priori.

Produce una rappresentazione gerarchica che può essere utile per comprendere le relazioni tra i dati.

Adatto a dataset di piccole e medie dimensioni.
```

#### Svantaggi:

```
Sensibile alla scelta della misura di distanza e del metodo di linkage.

Può essere computazionalmente costoso per dataset di grandi dimensioni. (quello divisivo)

Una volta effettuata una fusione, non è possibile separare i cluster, rendendo l'algoritmo meno flessibile in caso di errori.
```

## 3.1.2 K-means

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-means, dopo aver scalato le variabili numeriche del dataset.

```{r}
library(factoextra)
df <- scale(banknote[2:7])
```

Successivamente si determina il numero ottimale di cluster da utilizzare nell'algoritmo, impiegando il metodo Elbow.

```{r}
fviz_nbclust(df, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

E il metodo Silhouette.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Il primo metodo indica quattro come numero ottimale di cluster, mentre il secondo ne suggerisce due. Poiché il dataset è composto da 200 osservazioni, equamente suddivise tra banconote autentiche e contraffatte, si sceglie di impostare il numero di cluster a due e si procede con l'esecuzione dell'algoritmo.

Per prima cosa si osserva una matrice di grafici a dispersione, utile per analizzare le relazioni tra le variabili numeriche del dataset.

```{r}
set.seed(123)
km.res <- kmeans(df, 2, nstart = 25)
print(km.res)

aggregate(banknote, by=list(cluster=km.res$cluster), mean)

dd <- cbind(banknote, cluster = km.res$cluster)
head(dd)

km.res$cluster

km.res$size

km.res$centers

cl <- km.res$cluster
pairs(df, gap=0, pch=cl, col=c("red", "blue")[cl])
```

TODO: commentare

Ora si analizza la distribuzione dei due cluster.

```{r}
fviz_cluster(km.res, 
             data = df,
             palette = c("red", "blue"),
             ellipse.type = "euclid", 
             star.plot = TRUE,
             repel = TRUE,
             ggtheme = theme_minimal()
)
```

## 3.1.3 K-medoids

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-medoids con k impostato a 2.

Come in precedenza, si analizzano prima i grafici a dispersione e successivamente i due cluster.

```{r}
library(cluster)
head(df, n = 3)

pam.res <- pam(df, 2)
print(pam.res)

dd <- cbind(banknote, cluster = pam.res$cluster)
head(dd, n = 8)

pam.res$medoids

head(pam.res$clustering)

cl <- pam.res$clustering
pairs(df, gap=0, pch=cl, col=c("#00AFBB", "#FC4E07")[cl])
```

Come previsto, i cluster risultano molto simili.

```{r}
fviz_cluster(pam.res,
             palette = c("#00AFBB", "#FC4E07"),
             ellipse.type = "t",
             repel = TRUE,
             ggtheme = theme_classic()
)
```


## 3.1.4 Misture di Gaussiane

In questa sezione introduciamo l'applicazione del clustering basato su misture di gaussiane, utilizzando la funzione `Mclust` del pacchetto **mclust**. Questo approccio assume che i dati siano generati da una combinazione di distribuzioni gaussiane e, tramite l'algoritmo EM (Expectation-Maximization), stima i parametri di ciascuna componente. Il vantaggio principale di questo metodo è la capacità di modellare cluster con forme ellissoidali, permettendo di gestire anche cluster con sovrapposizioni. Inoltre, `Mclust` effettua una selezione automatica del numero ottimale di cluster e del modello di covarianza (parsimonioso) in base al criterio BIC, fornendo un'indicazione della bontà dell'adattamento e della complessità del modello.  


```{r}
library(mclust)
data(banknote)
dati <- banknote[, -1]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
Nonostante sappiamo che il dataset banknote presenta due classi reali (ad esempio, banconote fraudolente e non fraudolente), l'analisi con Mclust ha individuato come miglior modello un numero di cluster pari a 3. Inoltre, il modello selezionato è di tipo VEE (ovvero ellissoidale, con forma variabile, ma con volume ed orientamento uguali), in base al valore più alto di BIC evidenziato nel plot. È interessante notare come il BIC per 2 cluster risulti significativamente inferiore rispetto a quello per un numero maggiore di cluster, probabilmente perché sono state utilizzate tutte le variabili del dataset per costruire i cluster.  
Confrontiamo esplicitamente i valori di BIC e ICL per soluzioni con 2 e 3 cluster:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, -1]
dati_scaled <- scale(dati)
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```
Dai sommari ottenuti si osserva che entrambi i modelli, per k = 2 e k = 3, forniscono valori che suggeriscono una soluzione migliore per 3 cluster. I valori del BIC e, conseguentemente, anche quelli dell'ICL (che include una penalizzazione per l'incertezza nelle assegnazioni), indicano che la soluzione a 3 cluster è preferibile rispetto a quella a 2 cluster.  
Valutiamo l'effetto della scelta delle variabili sul modello. Utilizziamo invece solo le due variabili "Bottom" e "Diagonal", individuate in precedenza dagli scatterplot come le più discriminanti:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, c("Bottom", "Diagonal")]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
Anche in questo caso si nota che il valore di BIC è notevolmente più alto (migliore) rispetto al caso in cui sono state usate tutte le variabili. L'algoritmo suggerisce ancora un numero ottimale di cluster pari a 3, ma questa volta ha selezionato come modello parsimonioso il modello EVE (ellissoidale, con volume uguale e orientamento variabile). È importante osservare che in uno dei cluster sono presenti solamente 16 osservazioni, il che potrebbe indicare un gruppo di dati che, probabilmente, sono state erroneamente assegnate a un cluster separato.  
Verifichiamo ora se i valori di ICL sono in accordo con quelli del BIC confrontando esplicitamente le soluzioni per 2 e 3 cluster:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, c("Bottom", "Diagonal")]
dati_scaled <- scale(dati)
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```
In questo caso, i valori di BIC e ICL sono concordi: per un numero di cluster pari a 3 si ottiene un BIC di circa -845 e un ICL di -849, mentre per 2 cluster il BIC risulta attorno a -888 e l'ICL a -890. Questo dimostra che anche l'ICL, che penalizza ulteriormente le assegnazioni incerte, suggerisce che il modello ottimale è quello con 3 cluster.  
Adesso, procederemo ad utilizzare solamente le variabili estratte tramite la tecnica PCA.  


```{r}
library(mclust)
data(banknote)
#dati <- banknote[, c("Bottom", "Diagonal")]
#PCA
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```




```{r}
library(GGally)
ggpairs(banknote[,-1],upper = list(continuous = "density", combo = "box_no_facet"),
        lower = list(continuous = "points", combo = "dot_no_facet"),aes(colour=banknote$Status))
```


