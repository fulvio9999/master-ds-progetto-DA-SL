---
title: "Analisi di Clustering sul Dataset Banknote"
author: "D'Atri, Lombardi, Maiello"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.align = "center")

packages <- c("mclust", "reshape2", "ggplot2", "GGally", "dplyr", "clustertend", "cluster", "FNN", "FactoMineR", "factoextra", "caret", "pheatmap", "clusterCrit", "NbClust", "fpc", "scatterplot3d")

for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        install.packages(pkg, repos = "https://cran.rstudio.com/")
    }
}

```

# 1. Introduzione

Il presente progetto si propone di applicare diverse tecniche di clustering al dataset **banknote** al fine di identificare gruppi omogenei presenti nei dati e confrontare l'efficacia dei vari metodi. L'analisi intende mettere in pratica le metodologie apprese nel modulo di *Data Analysis & Statistical Learning*, in particolare, verranno trattati i seguenti algoritmi di clustering: **Clustering Gerarchico Agglomerativo e Divisivo**, **Clustering Partizionale (K-means e K-medoids)** e **Clustering Model-based (Mixture of Gaussian)**.  
 
Tali algoritmo sono stati valutati su tre diverse tipologie di dataset:  
- **Dataset Completo:** utilizzo di tutte le variabili disponibili.  
- **Dataset Ridotto:** uso esclusivo delle variabili “Bottom” e “Diagonal”, individuate come le più discriminative.  
- **Dataset Trasformato:** applicazione della PCA per ridurre le variabili a componenti che sintetizzano l’informazione.  
 
Per ciascuna configurazione, sono state valutate le performance dei metodi di clustering mediante metriche interne ed esterne.  
Il documento è organizzato nei seguenti capitoli:
 
1.  **Introduzione**: Presentazione del progetto, degli obiettivi e del contesto.  
2.  **Descrizione del Dataset e Analisi Esplorativa**: Esplorazione preliminare dei dati, analisi descrittiva e operazioni di preprocessing, tra cui la costruzione del dataset trasformato mediante la tecnica PCA.  
3.  **Applicazione delle Tecniche di Clustering**: Implementazione dei metodi di clustering (gerarchico, partizionale e model-based).  
4.  **Validazione e Visualizzazione dei Risultati**: Confronto dei risultati mediante metriche di validazione e visualizzazione dei cluster (con l'ausilio della PCA).  
5.  **Conclusioni e Sviluppi Futuri**: Sintesi dei risultati ottenuti e prospettive per ulteriori analisi.  

# 2. Descrizione e analisi del dataset

In questa sezione esamineremo il dataset **banknote** per comprenderne la struttura, le caratteristiche e le relazioni tra le variabili. Il dataset proviene dal pacchetto `mclust` e contiene misure che possono essere utilizzate per distinguere tra banconote autentiche e false (o per altri scopi diagnostici).
Il dataset contiene le seguenti variabili:

- Status: lo stato della banconota (autentica o contraffatta)

- Length: lunghezza della banconota (mm)

- Left: larghezza del bordo sinistro (mm)

- Right: larghezza del bordo destro (mm)

- Bottom: larghezza del margine inferiore (mm)

- Top: larghezza del margine superiore (mm)

- Diagonal: lunghezza della diagonale (mm)

Di seguito carichiamo tutte le librerie necessarie per il progetto.

```{r}
library("mclust")
library("reshape2")
library("ggplot2")
library("GGally")
library("dplyr")
library("clustertend")
library("FNN")
library("FactoMineR")
library("factoextra")
library("caret")
library("pheatmap")
library("clusterCrit")
library("NbClust")
library("fpc")
library("scatterplot3d")
```

Carichiamo il dataset.

```{r load-data}
data(banknote)
```

Visualizziamo le prime righe del dataset e la sua struttura.

```{r head}
head(banknote)
str(banknote)
```

Il dataset è composto da 200 osservazioni e 7 variabili, tutte di tipo numerico ad eccezione di 'Status' che è una variabile binaria che può assumere uno tra i seguenti valori: 'counterfeit' e 'genuine'.

Visualizziamo un riepilogo statistico delle variabili.

```{r summary}
summary(banknote)
```
Notiamo che la colonna 'Status' è perfettamente bilanciata.

Controlliamo se ci sono valori mancanti nelle colonne.

```{r missing values}
colSums(is.na(banknote))
```
Come possiamo vedere nessuna colonna presenta valori mancanti.  
Per avere una prima idea delle relazioni tra le variabili, creiamo una matrice di scatterplot. Utilizziamo la colonna 'Status' per colorare i punti.

```{r scatterplot}
library(GGally)
ggpairs(banknote[,-1],upper = list(continuous = "density", combo = "box_no_facet"),
        lower = list(continuous = "points", combo = "dot_no_facet"),aes(colour=banknote$Status))
```

TODO: sintetizzare
Lungo la diagonale sono visibili le distribuzioni univariate delle variabili. Si nota una sovrapposizione parziale tra i due gruppi di colore (Status), ma anche zone in cui uno dei due colori prevale.
Nei grafici di densità bivariata (metà diagonale superiore della matrice) e negli scatter plot (metà diagonale inferiore della matrice), si osservano alcune regioni in cui i punti di uno stesso colore tendono a concentrarsi, suggerendo cluster parzialmente distinti.
Nonostante la sovrapposizione in diverse aree, si intravedono regioni in cui un gruppo prevale. Ciò suggerisce che un algoritmo di classificazione o di clustering potrebbe distinguere parzialmente le due classi, anche se non in modo perfetto con una singola coppia di variabili.
È importante sottolineare che questo grafico mostra solo le relazioni a coppie. Ciò significa che l'analisi si limita a considerare due variabili alla volta, mentre una clusterizzazione netta potrebbe emergere solo quando si considerano contemporaneamente più variabili. In altre parole, anche se alcune coppie non evidenziano una separazione chiara, combinando più dimensioni si potrebbe scoprire una struttura clusterizzata più marcata.
Il fatto che in alcune proiezioni (coppie di variabili) si vedano zone di separazione o contorni distinti suggerisce che esistono sottostrutture nei dati. Notiamo infine che le variabili 'Bottom' e 'Diagonal' sembrerebbero separare in modo migliore il dataset rispetto alle altre variabili.

D'ora in avanti, per ciascun algoritmo di clustering che proveremo, verranno effettuate tre diverse analisi:

1. **Dataset completo**: utilizzando tutte le variabili disponibili nel dataset.

2. **Dataset ridotto**: considerando esclusivamente le variabili 'Diagonal' e 'Bottom'.

3. **Dataset trasformato**: Applicando l'Analisi delle Componenti Principali (PCA) per ridurre la dimensionalità.

Queste tre approcci permetteranno di valutare l'impatto delle diverse selezioni di variabili e della riduzione dimensionale sulle prestazioni e sui risultati degli algoritmi di clustering.  

## 2.1. Valutazione della tendenza al clustering

Prima di applicare gli algoritmi di clustering, è fondamentale verificare se il dataset presenta una naturale propensione alla formazione di gruppi. Utilizziamo specifiche misure statistiche per determinare se i dati sono strutturati in cluster o se, al contrario, sono distribuiti in modo casuale. Questi metodi permettono di valutare se il clustering è effettivamente significativo e utile per l'analisi del dataset.

### 2.1.1. Hopkins statistics

TODO: sintetizzare
L'Hopkins Statistic è una misura utilizzata per valutare la tendenza di un dataset alla clusterizzazione, confrontando la sua struttura con quella di un insieme di punti generati casualmente.
Assume valori compresi tra 0 e 1.
Valori vicini a 0 indicano che il dataset è altamente strutturato in cluster, suggerendo che l’applicazione di algoritmi di clustering può essere efficace.
Valori vicini a 0.5 o superiori suggeriscono che i dati sono distribuiti in modo casuale o uniforme, e il clustering potrebbe non essere significativo.
```{r}
banknote_num <- banknote[, -1]

banknote_num <- scale(banknote_num)

set.seed(987)

hopkins_stat <- hopkins(banknote_num, n = nrow(banknote_num) - 1)

random_df <- apply(banknote_num, 2, function(x){runif(length(x), min(x), max(x))})
random_df <- as.data.frame(random_df)

random_df <- scale(random_df)

set.seed(987)

hopkins_stat_rand <- hopkins(random_df, n = nrow(random_df)-1)

cat("Hopkins Statistics on Banknote dataset:", hopkins_stat$H, "\n")
cat("Hopkins Statistics on Random dataset:", hopkins_stat_rand$H, "\n")
```
Un valore di Hopkins pari a 0.2519264, essendo inferiore a 0.5, indica una tendenza alla clusterizzazione. In altre parole, le distanze tra i punti reali sono molto più ridotte rispetto a quelle dei punti generati casualmente, suggerendo la presenza di gruppi ben definiti nel dataset. Questo risultato supporta l'ipotesi che il dataset sia strutturato in cluster distinti.

### 2.1.2. Visual Assessment of cluster Tendency (VAT)
L'algoritmo VAT produce una matrice di dissimilarità ordinata, ottenuta riorganizzando le righe e le colonne della matrice delle distanze per evidenziare eventuali blocchi di punti “ravvicinati” (ossia potenziali cluster). Visualizziamo la matrice con l'intento di verificare se si formano lungo la diagonale secondaria del grafico, dei blocchi rossi che rappresentano regioni di osservazioni che condividono una forte similarità.

```{r fig.width = 15, fig.height = 7}

p1 <- fviz_dist(dist(banknote), show_labels = FALSE) + labs(title = "Banknote data")
p2 <- fviz_dist(dist(random_df), show_labels = FALSE) + labs(title = "Random data")

grid.arrange(p1, p2, ncol = 2, widths = c(6, 6))
```
TODO: sintetizzare
Nei grafici VAT il rosso indica alta similarità (ossia bassa dissimilarità), mentre il blu indica bassa similarità (ossia alta dissimilarità).
Relativamente al grafico del dataset banknote, notiamo che lungo la diagonale sono presenti regioni rosse che indicano dati simili tra loro, suggerendo la presenza di cluster.
La presenza di zone blu che evidenziano una maggiore dissimilarità tra gruppi di dati, supporta l'ipotesi che esistano cluster distinti nel dataset.
Inoltre la presenza di transizioni marcate dal rosso al blu conferma che esistono confini ben definiti tra i cluster.
Notiamo invece come nel dataset randomico non c'è alcuna struttura a cluster evidente. I dati appaiono disordinati e non presenteranno raggruppamenti chiari, il che indica che un clustering non ha senso o che i dati non sono adatti per essere separati in gruppi distinti.

Possiamo concludere che il dataset Banknote ha una tendezza alla clusterizzazione, procediamo perciò con il calcolo della PCA e successivamente alla clusterizzazione.

## 2.2. Riduzione della dimensionalità con PCA

In questa sezione applichiamo la PCA sul dataset, considerando solo le colonne numeriche. Anche se le variabili sono tutte misurate in millimetri e quindi nella stessa scala, applichiamo comunque la standardizzazione per evitare che differenze nella dispersione (variabili con varianze maggiori) influenzino in modo sproporzionato i risultati della PCA.
```{r}
banknote_num <- banknote[, -1]
head(banknote_num)
pca_result <- PCA(banknote_num, scale = TRUE, graph = FALSE)
```

Estraiamo gli autovalori e calcoliamo sia la varianza spiegata per ciascuna componente sia la varianza cumulativa. Queste informazioni ci serviranno per decidere il numero ottimale di componenti principali da mantenere.
```{r}
eigenvalues <- get_eigenvalue(pca_result)[,1]
explained_variance <- eigenvalues / sum(eigenvalues)
cumulative_variance <- cumsum(explained_variance)

print(eigenvalues)
print(explained_variance)
print(cumulative_variance)
```
Abbiamo ora a disposizione gli autovalori e le informazioni sulla varianza, utili per valutare l'importanza di ciascuna componente principale.

Utilizziamo la Kaiser Rule, una tecnica utilizzata per determinare il numero di componenti principali da mantenere quando si esegue l'analisi delle componenti principali. La regola si basa sull'idea di conservare solo le componenti con una varianza spiegata maggiore di 1. 
Questo criterio si applica al valore proprio (eigenvalue) delle componenti principali.

```{r kaiser rule}
kaiser_components <- sum(eigenvalues > 1)
cat("Numero di componenti selezionati con la Kaiser rule:", kaiser_components, "\n")
```
Otteniamo 2 come numero di componenti principali da mantenere, ciò significa che le due componenti principali con valori propri superiori a 1 spiegano una quantità di varianza maggiore di quella che verrebbe spiegata da una singola variabile originale del dataset.

Un ulteriore criterio è quello di selezionare il numero minimo di componenti la cui somma di varianza spiegata (varianza comulativa), raggiunge almeno l'80% del totale. In questo modo garantiamo una buona rappresentazione dei dati.
```{r threshold}
variance_threshold <- min(which(cumulative_variance >= 0.80))

cat("Numero di componenti principali che spiegano l'80% della varianza comulativa:", variance_threshold, "\n")

fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))
```
Nel grafico vediamo la varianza spiegata da ciascuna componente. Con le prime tre componenti principali riusciamo a spiegare l'84.9% della varianza cumulativa.

Vediamo infine un ultimo metodo per scegliere il numero di componenti principali: l'elbow method.
Mediante un grafico scree plot visualizziamo gli autovalori per ciascuna componente e individuiamo "il gomito", ovvero il punto nel grafico in cui la diminuzione degli autovalori rallenta. Tale punto può essere interpretato come un'indicazione del numero ottimale di componenti. Includiamo nel grafico anche due linee verticali per evidenziare i criteri della varianza cumulativa (rosso) e della Kaiser Rule (blu).
```{r elbow}
scree_plot <- data.frame(PC = 1:length(eigenvalues), Eigenvalue = eigenvalues)

ggplot(scree_plot, aes(x = PC, y = Eigenvalue)) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = variance_threshold, linetype = "dashed", color = "red") +
    geom_vline(xintercept = kaiser_components, linetype = "dotted", color = "blue") +
    labs(title = "Scree Plot for PCA", x = "Principal Component", y = "Eigenvalue") +
    theme_minimal()
```

Nel grafico il gomito sembra collocarsi intorno alla quarta componente. I tre criteri danno perciò risultati differenti, per mantenere un equilibrio tra interpretabilità e preservazione della varianza, si potrebbe optare per 3 componenti.  
Di seguito viene costruito il nuovo dataset in cui le variabili sono sostituite con le prime 3 PCA, che verrà usato più avanti nelle analisi.

```{r}
pca_data <- as.data.frame(pca_result$ind$coord[, 1:3])
colnames(pca_data) <- c("PCA1", "PCA2", "PCA3")
banknote_pca <- cbind(pca_data, Status = banknote$Status)
banknote_pca <- banknote_pca %>% dplyr::select(Status, everything())
head(banknote_pca)
```

Per comprendere meglio il contributo delle variabili alle componenti principali, realizziamo una mappa di correlazione. Questo ci aiuta a capire come le variabili si relazionano alle componenti principali.
```{r}
var <- get_pca_var(pca_result)

fviz_pca_var(pca_result, col.var = "contrib",
            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```
Questo passaggio ci offre una visione immediata di come ogni variabile contribuisce alle componenti principali e della loro interrelazione.

Approfondiamo l'analisi mostrando i contributi delle variabili attraverso diversi metodi grafici, in modo da identificare quali variabili influenzano maggiormente ciascuna componente.
```{r}
corrplot(var$contrib, is.corr=FALSE)
```
L'analisi dei contributi evidenzia il peso relativo di ciascuna variabile nelle componenti principali, utile per interpretare i risultati della PCA.

Visualizziamo i contributi delle variabili specificamente per la prima componente principale.
```{r}
fviz_contrib(pca_result, choice = "var", axes = 1)
```
Similmente al passaggio precedente, visualizziamo i contributi delle variabili per la seconda componente principale.
```{r}
fviz_contrib(pca_result, choice = "var", axes = 2)
```

# 3. Clustering gerarchico

## 3.1 Clustering Gerarchico Agglomerativo

Il **Clustering Gerarchico Agglomerativo (HAC)** è una tecnica di clustering che costruisce una gerarchia di cluster attraverso un approccio "bottom-up". Inizia considerando ogni punto dati come un cluster separato e, successivamente, unisce iterativamente i cluster più
simili fino a formare un unico cluster che racchiude tutti i dati. Questo metodo non richiede la specifica del numero di cluster a priori e produce una rappresentazione ad albero, nota come **dendrogramma**, che facilita la visualizzazione delle relazioni tra i cluster.

Per applicare il clustering gerarchico agglomerativo al nostro dataset, seguiamo questi passaggi:

1.  **Scaling**: Standardizziamo i dati.

2.  **Calcolo della Matrice di Distanza**: Determiniamo le distanze tra tutte le coppie di punti nel dataset. Una misura comune è la distanza euclidea.

3.  **Applicazione del clustering agglomerativo**: Utilizziamo la funzione `hclust()` per eseguire il clustering gerarchico e generare il dendrogramma.

4.  **Taglio del Dendrogramma**: Decidiamo il numero ottimale di cluster utilizzando varie tecniche, e tagliamo il dendrogramma al livello appropriato.

5.  **Valutazione**: Valutiamo i risultati del clustering utilizzando varie metriche.

### 3.1.1. Dataset completo

Scaliamo il dataset e calcoliamo la relativa matrice di dissimilatità mediante distanza euclidea.

```{r dissimilar matrix}
df <- scale(banknote_num)
distanze <- dist(df, method = "euclidean")
diss_matrix <- as.matrix(distanze)
```

Applichiamo il clustering con metodo di linkage 'ward' che generalmente insieme all'average linkage, performa meglio.

```{r hierarchical agglomerative clustering hclust}
hclust_clustering <- hclust(distanze, method = "ward.D2")
```
Guardiamo il dendrogramma cercando di identificare il numero ottimale di cluster. Utilizzeremo poi anche altri metodi che ci premetteranno di prendere una scelta migliore.

#### 3.1.1.1. Scelta del miglior numero di cluster k

```{r hierarchical agglomerative clustering dend}
fviz_dend(hclust_clustering, show_labels = FALSE, as.ggplot = TRUE)
```
TODO: sintetizzare
Il dendrogramma risultante mostra come le osservazioni vengono unite progressivamente in cluster. L'altezza dei rami indica la distanza a cui avviene la fusione: rami più bassi corrispondono a fusioni tra punti o cluster molto simili. Analizzando il dendrogramma, possiamo decidere il
numero di cluster tagliando l'albero a un'altezza che separa i gruppi in modo significativo.
Dall'osservazione del dendrogramma, si nota un salto particolarmente ampio a un'altezza intorno a 30, il che indica che fino a quella soglia i gruppi sono relativamente compatti e ben separati. Tagliare il dendrogramma a quel livello produrrebbe 2 grandi cluster. Volendo una suddivisione più fine, si può individuare un altro salto meno marcato (intorno a 15) che porterebbe a 3 cluster.
Procediamo con l'analisi del numero ottimale di cluster utilizzando altri metodi.

Osserviamo l'andamento del Total Within Sum of Squares (WSS) rispetto a k, cercando di individuare il punto in cui la curva inizia a “piegarsi” più marcatamente, cioè dove l'aggiunta di ulteriori cluster non porta a una riduzione significativa del WSS. Di seguito il grafico Elbow.
```{r hierarchical agglomerative clustering elbow}
fviz_nbclust(df, FUN = hcut, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2) +
    labs(subtitle = "Elbow method")
```
Tra k = 1 e k = 2, la diminuzione del WSS è molto forte (ciò è normale, perché si passa da un unico cluster a due). Tra k = 2 e k = 3, c'è ancora un calo netto del WSS.
Oltre k = 3, la pendenza inizia a ridursi gradualmente. Dal grafico, sembra che il calo principale avvenga per k = 3, mentre oltre si riduce più lentamente.
Per k = 3 si ha un notevole miglioramento rispetto a 2, ma passando a 4 (e oltre) il guadagno in termini di riduzione del WSS diventa più contenuto.

Analizziamo ora il grafico dell'Average Silhouette Width.
```{r hierarchical agglomerative clustering hclust silhouette}
fviz_nbclust(df, FUN = hcut, method = "silhouette") +
    labs(subtitle = "Silhouette method")
```
TODO: sintetizzare
L'indice di silhouette risulta più alto in corrispondenza di k = 2. Questo suggerisce che la separazione tra i cluster e la coesione interna sia massima quando si dividono i dati in due gruppi.
Con k = 3, l'indice diminuisce, e oltre tale valore rimane su livelli inferiori (intorno a 0.2-0.3), senza tornare ai valori di picco.

Osserviamo il grafico del Gap Statistic che confronta la dispersione all'interno dei cluster ottenuta dal clustering, con quella attesa in un dataset generato casualmente (quindi privo di gruppi).
Ossia calcola la differenza (gap) tra il log della dispersione osservata e quella media attesa sotto una distribuzione nulla.
```{r hierarchical agglomerative clustering hclust gap statistics}
set.seed(123)
fviz_nbclust(df, FUN = hcut, method = "gap_stat", nboot = 500, verbose = FALSE)+
    labs(subtitle = "Gap statistic method")
```
TODO: sintetizzare
Nel grafico, la linea che rappresenta il punto in cui il Gap Statistic non aumenta più in maniera significativa, è su 3 cluster, il che indica che quando i dati vengono divisi in 3 gruppi la struttura di clustering risulta migliore rispetto a quella attesa in un dataset casuale. In altre parole, il modello a 3 cluster spiega meglio la struttura intrinseca dei dati.

Utilizziamo infine il pacchetto 'NbClust' che valuta il numero ottimale di cluster utilizzando numerosi indici interni. Tra questi, alcuni indici come l'indice di Hubert e il Dindex vengono visualizzati graficamente.
```{r hierarchical agglomerative clustering hclust NbClust}
library("NbClust")

nb <- NbClust(df, distance = "euclidean", min.nc = 2, max.nc= 10, method = "ward.D2")
```
La decisione finale basata sulla majority rule suggerisce 3 cluster, indicando che, complessivamente, questa soluzione offre una migliore separazione e coesione interna rispetto alle altre soluzioni.

Complessivamente la maggioranza dei metodi analizzati (elbow, gap statistic, nbclust) converge su una soluzione a 3 cluster.
Pertanto optiamo per una scelta di "maggioranza" e scegliamo perciò un numero di cluster pari a 3.

Procediamo con il taglio del dendogramma in modo da ottenere 3 cluster. Successivamente visualizziamo una tabella di frequenza che mostra il numero di osservazioni assegnate a ciascun cluster, permettendoci di verificare la distribuzione dei dati tra i 3 cluster ottenuti.
```{r hierarchical agglomerative clustering hclust cut}
cluster_cut <- cutree(hclust_clustering, k = 3)

table(cluster_cut)

fviz_dend(hclust_clustering, k = 3,
        cex = 0.5,
        k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
        color_labels_by_k = TRUE,
        rect = TRUE
)
```
La maggior parte dei dati è stata assegnata ai primi due cluster, mentre il terzo cluster contiene un numero significativamente minore di osservazioni. Questo potrebbe suggerire che il cluster 3 rappresenta un gruppo più piccolo, con caratteristiche potenzialmente diverse o più specifiche rispetto ai gruppi più grandi.

Vediamo i risultati di clustering nello spazio originale, tramite la matrice di scatterplot a coppie con i punti colorati in base ai cluster di appartenenza.
```{r hierarchical agglomerative clustering hclust pairs}
ggpairs(df, aes(color = as.factor(cluster_cut))) +
  ggtitle("Matrice Pairwise Scatterplot con Cluster")
```

#### 3.1.1.2. Validazione

Valutiamo ora la qualità dei risultati del nostro algoritmo di clustering mediante quattro metriche: Average Silhouette Width, Dunn Index, Corrected Rand Index e  Meila's Variation of Information Index.

Iniziamo con la metrica della silhouette media (calcolata come la media di s(i) su tutte le osservazioni), che fornisce un'indicazione complessiva della qualità del clustering: si auspica un valore il più vicino ad 1 possibile.
```{r hierarchical agglomerative clustering hclust cut sil}
sil <- silhouette(cluster_cut, distanze)

fviz_silhouette(sil, palette = "jco", ggtheme = theme_classic())
```
Un valore di 0.31 suggerisce che la struttura dei cluster non è particolarmente forte: c'è una separazione moderata, ma c'è anche una certa sovrapposizione tra i cluster.

Analizziamo ora il Dunn Index, che valuta la separazione tra i cluster rispetto alla loro compattezza, cioè quanto i cluster siano ben separati e internamente compatti.
Se i dati contengono cluster compatti e ben separati, ci si aspetta che il diametro dei cluster sia piccolo e che la distanza tra i cluster sia grande.
Pertanto l'indice di Dunn deve essere massimizzato.

```{r hierarchical agglomerative clustering hclust dunn}

dunn_index <- intCriteria(as.matrix(df), cluster_cut, "Dunn")$dunn

print(paste("Dunn Index:", dunn_index))
```

Un Dunn Index di 0.125 indica che i cluster hanno una separazione piuttosto bassa rispetto alla loro coesione interna. Una bassa separazione tra cluster suggerisce possibili problemi di sovrapposizione.

Analizziamo il Corrected Rand Index che misura quanto bene il clustering ottenuto corrisponde a una partizione di riferimento. Nel nostro caso utilizziamo la colonna 'Status' del dataset.
Ed infine il Meila's Variation of Information (VI) Index che misura quanto due cluster differiscono tra loro in termini di entropia. A differenza del Corrected Rand Index, che valuta la similarità, il VI Index misura la distanza tra due partizioni (va perciò minimizzato).
```{r hierarchical agglomerative clustering hclust cri}
library("fpc")

status <- as.numeric(banknote$Status)

clust_stats <- cluster.stats(d = dist(df), status, cluster_cut)

print(paste("Corrected Rand Index:", clust_stats$corrected.rand))

print(paste("Meila's Index:", clust_stats$vi))
```
TODO: sintetizzare
Un Corrected Rand Index di 0.792 indica che il clustering è molto simile alla vera classificazione dei dati.
Un Meila's Variation of Information Index di 0.359 è un valore relativamente basso, il che significa che i cluster trovati non sono troppo distanti dalla struttura effettiva dei dati.
Il fatto che il Corrected Rand Index sia alto (0.792) e il VI Index sia basso conferma che il clustering ha una buona coerenza con la partizione reale.
Tuttavia, poiché Dunn Index e Silhouette suggeriscono che la separazione tra cluster è debole, questo potrebbe indicare sovrapposizione tra i cluster, ma senza compromettere eccessivamente la qualità dell'assegnazione.

#### 3.2.1.3. Ottimizzazione

TODO: sintetizzare
Ottimizziamo il clustering esplorando diverse combinazioni di metodi di linkage e numeri di cluster. Per ciascuna combinazione, calcoliamo le quattro metriche di valutazione precedentemente analizzate, ottenendo un dataframe con tutti i risultati.
Successivamente, identifichiamo il metodo di clustering migliore per ciascuna metrica. Poiché le diverse metriche potrebbero suggerire metodi differenti, assegniamo un ranking a ciascuna di esse e calcoliamo un ranking totale sommando i punteggi (invertendo il criterio per le metriche da massimizzare).
Infine, ordiniamo i risultati e selezioniamo la combinazione con il punteggio complessivo più basso, individuando così il miglior clustering in base a tutte le metriche disponibili.

```{r hierarchical agglomerative clustering linkage methods}
library(dplyr)

evaluate_clustering <- function(df, distanze, status, linkage_methods = c("ward.D2", "single", "complete", "average", "centroid"), min_k = 2, max_k = 10) {
    results_all <- data.frame(Method = character(),
                            k = numeric(),
                            Silhouette = numeric(),
                            Dunn = numeric(),
                            Corrected_Rand = numeric(),
                            VI = numeric(),
                            stringsAsFactors = FALSE)

    for(method in linkage_methods) {
        hclust_obj <- hclust(distanze, method = method)
        
        for(k in min_k:max_k) {
            cluster_cut <- cutree(hclust_obj, k = k)
            
            sil <- silhouette(cluster_cut, distanze)
            avg_sil <- mean(sil[, 3])
            
            dunn_index <- intCriteria(as.matrix(df), cluster_cut, "Dunn")$dunn
            
            clust_stats <- cluster.stats(distanze, status, cluster_cut)

            corrected_rand <- clust_stats$corrected.rand

            vi_val <- clust_stats$vi
            
            results_all <- rbind(results_all, data.frame(Method = method,
                                                        k = k,
                                                        Silhouette = avg_sil,
                                                        Dunn = dunn_index,
                                                        Corrected_Rand = corrected_rand,
                                                        VI = vi_val,
                                                        stringsAsFactors = FALSE))
        }
    }
    return(results_all)
}

results_all <- evaluate_clustering(df, distanze, status)

best_silhouette <- results_all %>% group_by(Method) %>% filter(Silhouette == max(Silhouette))
print(best_silhouette)

best_dunn <- results_all %>% group_by(Method) %>% filter(Dunn == max(Dunn))
print(best_dunn)

best_corrected_rand <- results_all %>% group_by(Method) %>% filter(Corrected_Rand == max(Corrected_Rand))
print(best_corrected_rand)

best_vi <- results_all %>% group_by(Method) %>% filter(VI == min(VI))
print(best_vi)
```
```{r hierarchical agglomerative clustering best}
library(dplyr)

results_ranked <- results_all %>%
  mutate(
    rank_sil = rank(-Silhouette, ties.method = "min"),
    rank_dunn = rank(-Dunn, ties.method = "min"),
    rank_corrected_rand = rank(-Corrected_Rand, ties.method = "min"),
    
    rank_vi = rank(VI, ties.method = "min"),
    
    total_rank = rank_sil + rank_dunn + rank_corrected_rand + rank_vi
  )

best_overall <- results_ranked %>% arrange(total_rank) %>% head(1)
print(best_overall)
```
Il risultato mostra che il clustering ottimale secondo queste metriche è Ward.D2 con 2 cluster, poiché bilancia al meglio la coesione, la separazione e la corrispondenza con la verità di riferimento.
La combinazione scelta restituisce i valori migliori possibili per le metriche Silhouette, Corrected Rand Index e VI (Rank 1), mentre risulta meno performante rispetto ad altri metodi, ma compensato dagli altri punteggi, per il Dunn Index (Rank 21).

Effettuiamo il clustering con la combinazione risultante e visualizziamo mediante il grafico i cluster ottenuti.
```{r hierarchical agglomerative clustering eclust}
final <- eclust(df, "hclust", k = 2, hc_metric = "euclidean", hc_method = "ward.D2", graph = FALSE)

fviz_cluster(final, geom = "point", ellipse.type = "norm", palette="jco", ggtheme = theme_minimal())
```

### 3.1.2. Dataset tradformato

Vediamo come cambia il clustering se invece che considerare tutte le variabili del dataset, consideriamo solo le tre componenti principali trovate prima.

```{r hierarchical agglomerative clustering pca best}

banknote_pca_num <- banknote_pca[,-1]

results_all_pca <- evaluate_clustering(banknote_pca_num, dist(banknote_pca_num), status)

results_ranked_pca <- results_all_pca %>%
  mutate(
    rank_sil = rank(-Silhouette, ties.method = "min"),
    rank_dunn = rank(-Dunn, ties.method = "min"),
    rank_corrected_rand = rank(-Corrected_Rand, ties.method = "min"),
    rank_vi = rank(VI, ties.method = "min"),
    
    total_rank = rank_sil + rank_dunn + rank_corrected_rand + rank_vi
  )

best_overall_pca <- results_ranked_pca %>% arrange(total_rank) %>% head(1)
print(best_overall_pca)
```

TODO: sintetizzare
Il risultato indica che, applicando il clustering agglomerativo con numero di cluster pari a 2 e metodo di linkage "average" al dataset ridotto a tre componenti principali, otteniamo la combinazione migliore in base ai 4 indici di valutazione, anche se i risultati presentano alcune peculiarità.
Rispetto alle altre combinazioni testate, la silhouette e la VI hanno ottenuto i migliori punteggi (rank 1), il Corrected Rand è quasi il migliore (rank 2), invece l'indice Dunn, in questa combinazione, non performa bene rispetto ad altre configurazioni testate (rank 27).
Rispetto al clustering agglomerativo applicato su tutte le variabili del dataset il clustering sul dataset ridotto mostra:
- Un indice silhouette leggermente migliore (0.431) rispetto a quello su tutte le variabili (0.375), suggerendo che i punti sono, mediamente, meglio raggruppati all'interno dei cluster nel caso PCA.
- Il clustering su tutte le variabili ha un indice Dunn migliore (0.211 vs 0.106), il che significa che la separazione minima tra cluster è maggiore e i cluster risultano più distinti.
- Entrambe le soluzioni mostrano valori molto alti del Corrected Rand e bassi del VI, ma la soluzione con tutte le variabili è leggermente superiore (0.960 vs 0.902 e 0.098 vs 0.200), indicando una migliore aderenza ad una struttura di riferimento o una maggiore stabilità del clustering.

Mentre l'approccio con PCA (riduzione a 3 componenti) offre cluster con una buona compattezza (indice silhouette migliore), il clustering basato su tutte le variabili, utilizzando il metodo ward.D2, risulta complessivamente superiore per quanto riguarda la separazione tra cluster e la corrispondenza con una struttura di riferimento (migliori valori di Dunn, Corrected Rand e VI).

Vediamo graficamente come sono separati i due cluster.
```{r hierarchical agglomerative clustering pca eclust}
library(scatterplot3d)

final_pca <- eclust(banknote_pca_num, "hclust", k = 2, hc_metric = "euclidean", hc_method = "average", graph = FALSE)

clusters <- final_pca$cluster

scatterplot3d(banknote_pca_num,
              color = clusters,
              pch = 19,
              main = "Clustering su PCA (PC1, PC2, PC3)",
              xlab = "PC1", ylab = "PC2", zlab = "PC3")

legend("topright", legend = unique(clusters), col = unique(clusters), pch = 19, title = "Cluster")

```

### 3.1.3. Dataset ridotto

Vediamo infine come cambia il clustering considerando solo le variabili 'Bottom' e 'Diagonal' che sembravano suddividere meglio i dati in gruppi.

```{r hierarchical agglomerative clustering bottom e diagonal}

banknote_bottom_diagonal <- banknote %>% select(Bottom, Diagonal)

banknote_bd_scaled <- scale(banknote_bottom_diagonal)

results_all_bd <- evaluate_clustering(banknote_bd_scaled, dist(banknote_bd_scaled), status)

results_ranked_bd <- results_all_bd %>%
  mutate(
    rank_sil = rank(-Silhouette, ties.method = "min"),
    rank_dunn = rank(-Dunn, ties.method = "min"),
    rank_corrected_rand = rank(-Corrected_Rand, ties.method = "min"),
    rank_vi = rank(VI, ties.method = "min"),
    
    total_rank = rank_sil + rank_dunn + rank_corrected_rand + rank_vi
  )

best_overall_bd <- results_ranked_bd %>% arrange(total_rank) %>% head(1)
print(best_overall_bd)
```
TODO: sintetizzare
In questo caso la combinazione migliore è data da metodo di linkage 'ward.D2' e numero cluster pari a 2. Notiamo che in questo caso che il valore Silhouette 0.6245 è più alto rispetto agli altri due approcci (0.431 per PCA e 0.375 usando tutte le variabili), il che indica che i punti all'interno dei cluster sono ben raggruppati e distinti l'uno dall'altro.
Ciò supporta l'idea iniziale che queste due variabili siano in grado di dividere bene il dataset.
Il Dunn index risulta più basso rispetto a quello ottenuto con tutte le variabili (0.211) o con la PCA (0.106), il che potrebbe significare che alcuni cluster risultano più vicini o con una certa dispersione interna.
Il Corrected Rand 0.9602 rimane molto elevato, segnalando una forte coerenza con la struttura di riferimento attesa. L'indice VI è basso, confermando una buona similarità o una discreta aderenza a una struttura di riferimento.

Visualizziamo come il dataset è stato diviso in cluster.
```{r hierarchical agglomerative clustering bottom e diagonal eclust}
final_bd <- eclust(banknote_bd_scaled, "hclust", k = 2, hc_metric = "euclidean", hc_method = "ward.D2", graph = FALSE)

fviz_cluster(final_bd, geom = "point", ellipse.type = "norm", palette="jco", ggtheme = theme_minimal())
```

# 4. Clustering Partizionale

TODO: inserire commento introduttivo

## 4.1. K-Means

TODO: inserire commento introduttivo

### 4.1.1. Dataset completo

TODO: inserire commento introduttivo

#### 4.1.1.1. Scelta del miglior numero di cluster k

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-means, dopo aver scalato le variabili numeriche del dataset.

```{r}
library(factoextra)
df <- scale(banknote[, -1])
```

Successivamente si determina il numero ottimale di cluster da utilizzare nell'algoritmo, impiegando il metodo Elbow.

```{r}
fviz_nbclust(df, kmeans, method = "wss") +
  geom_vline(xintercept = 2, linetype = 2)+
  labs(subtitle = "Elbow method")
```

Il metodo Silhouette.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Ed infine la GAP analysis:
```{r}
set.seed(123)
fviz_nbclust(df, kmeans, nstart = 25, method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")
```

Il primo metodo e il secondo indicano 2 come numero ottimale di cluster, il terzo ne suggerisce 3. Poiché il dataset è composto da 200 osservazioni, equamente suddivise tra banconote autentiche e contraffatte, si sceglie di impostare il numero di cluster a due e si procede con l'esecuzione dell'algoritmo.

#### 4.1.1.2. Validazione
TODO: inserire questa parte, prendere spunto dal clustering gerarchico

### 4.1.2 Dataset ridotto

#### 4.1.2.1. Scelta del miglior numero di cluster k

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-means, dopo aver scalato le variabili numeriche del dataset.

```{r}
df_ridotto <- df[, c("Bottom", "Diagonal")]
```

TODO: Successivamente si determina il numero ottimale di cluster da utilizzare nell'algoritmo, impiegando il metodo Elbow.

```{r}
fviz_nbclust(df_ridotto, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

TODO: Il metodo Silhouette.

```{r}
fviz_nbclust(df_ridotto, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

TODO: Ed infine la GAP analysis:

```{r}
set.seed(123)
fviz_nbclust(df_ridotto, kmeans, nstart = 25, method = "gap_stat", nboot = 500)+
  labs(subtitle = "Gap statistic method")
```

TODO: Il primo metodo indica quattro come numero ottimale di cluster, il secondo ne suggerisce due mentre la gap statistic tre. Poiché il dataset è composto da 200 osservazioni, equamente suddivise tra banconote autentiche e contraffatte, si sceglie di impostare il numero di cluster a due e si procede con l'esecuzione dell'algoritmo.

#### 4.1.2.2. Validazione
TODO: da fare

### 4.1.3 Dataset trasformato

#### 4.1.3.1. Scelta del miglior numero di cluster k

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-means, dopo aver scalato le variabili numeriche del dataset.

```{r}
df_pca <- scale(banknote_pca[, -1])
```

TODO: Successivamente si determina il numero ottimale di cluster da utilizzare nell'algoritmo, impiegando il metodo Elbow.

```{r}
fviz_nbclust(df_pca, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

TODO: Il metodo Silhouette.

```{r}
fviz_nbclust(df_pca, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

TODO: Ed infine la GAP analysis:

```{r}
set.seed(123)
fviz_nbclust(df_pca, kmeans, nstart = 25, method = "gap_stat", nboot = 500)+
  labs(subtitle = "Gap statistic method")
```

TODO: Il primo metodo indica quattro come numero ottimale di cluster, il secondo ne suggerisce due mentre la gap statistic tre. Poiché il dataset è composto da 200 osservazioni, equamente suddivise tra banconote autentiche e contraffatte, si sceglie di impostare il numero di cluster a due e si procede con l'esecuzione dell'algoritmo.

#### 4.1.3.2. Validazione
TODO: da fare

## 4.2 K-Medoids 
TODO: da rivedere tutto

### 4.2.1 Dataset completo

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-medoids con k impostato a 2.

Come in precedenza, si analizzano prima i grafici a dispersione e successivamente i due cluster.

```{r}
library(cluster)
head(df, n = 3)

pam.res <- pam(df, 2)
print(pam.res)

dd <- cbind(banknote, cluster = pam.res$cluster)
head(dd, n = 8)

pam.res$medoids

head(pam.res$clustering)

cl <- pam.res$clustering
pairs(df, gap=0, pch=cl, col=c("#00AFBB", "#FC4E07")[cl])
```

Come previsto, i cluster risultano molto simili.

```{r}
fviz_cluster(pam.res,
             palette = c("#00AFBB", "#FC4E07"),
             ellipse.type = "t",
             repel = TRUE,
             ggtheme = theme_classic()
)
```

### 4.2.2 Dataset ridotto

```{r}
df_ridotto <- df[, c("Bottom", "Diagonal")]
pam.res <- pam(df_ridotto, 2)
print(pam.res)

dd <- cbind(banknote, cluster = pam.res$cluster)
head(dd, n = 8)

pam.res$medoids

head(pam.res$clustering)

cl <- pam.res$clustering
pairs(df, gap=0, pch=cl, col=c("#00AFBB", "#FC4E07")[cl])
```

Come previsto, i cluster risultano molto simili.

```{r}
fviz_cluster(pam.res,
             palette = c("#00AFBB", "#FC4E07"),
             ellipse.type = "t",
             repel = TRUE,
             ggtheme = theme_classic()
)
```

### 4.2.3 Dataset trasformato

```{r}
df_pca <- scale(banknote_pca[, -1])
pam.res <- pam(df_pca, 2)
print(pam.res)

dd <- cbind(banknote, cluster = pam.res$cluster)
head(dd, n = 8)

pam.res$medoids

head(pam.res$clustering)

cl <- pam.res$clustering
pairs(df, gap=0, pch=cl, col=c("#00AFBB", "#FC4E07")[cl])
```

Come previsto, i cluster risultano molto simili.

```{r}
fviz_cluster(pam.res,
             palette = c("#00AFBB", "#FC4E07"),
             ellipse.type = "t",
             repel = TRUE,
             ggtheme = theme_classic()
)
```

# 5. Clustering Model-based: Mixture of Gaussian

In questa sezione introduciamo l'applicazione del clustering basato su misture di gaussiane, utilizzando la funzione `Mclust` del pacchetto **mclust**. Questo approccio assume che i dati siano generati da una combinazione di distribuzioni gaussiane e, tramite l'algoritmo EM (Expectation-Maximization), stima i parametri di ciascuna componente. Il vantaggio principale di questo metodo è la capacità di modellare cluster con forme ellissoidali, permettendo di gestire anche cluster con sovrapposizioni. Inoltre, `Mclust` effettua una selezione automatica del numero ottimale di cluster e del modello di covarianza (parsimonioso) in base al criterio BIC, fornendo un'indicazione della bontà dell'adattamento e della complessità del modello.  


```{r}
library(mclust)
data(banknote)
dati <- banknote[, -1]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
Nonostante sappiamo che il dataset banknote presenta due classi reali (ad esempio, banconote fraudolente e non fraudolente), l'analisi con Mclust ha individuato come miglior modello un numero di cluster pari a 3. Inoltre, il modello selezionato è di tipo VEE (ovvero ellissoidale, con forma variabile, ma con volume ed orientamento uguali), in base al valore più alto di BIC evidenziato nel plot. È interessante notare come il BIC per 2 cluster risulti significativamente inferiore rispetto a quello per un numero maggiore di cluster, probabilmente perché sono state utilizzate tutte le variabili del dataset per costruire i cluster.  
Confrontiamo esplicitamente i valori di BIC e ICL per soluzioni con 2 e 3 cluster:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, -1]
dati_scaled <- scale(dati)
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```
Dai sommari ottenuti si osserva che entrambi i modelli, per k = 2 e k = 3, forniscono valori che suggeriscono una soluzione migliore per 3 cluster. I valori del BIC e, conseguentemente, anche quelli dell'ICL (che include una penalizzazione per l'incertezza nelle assegnazioni), indicano che la soluzione a 3 cluster è preferibile rispetto a quella a 2 cluster.  
Valutiamo l'effetto della scelta delle variabili sul modello. Utilizziamo invece solo le due variabili "Bottom" e "Diagonal", individuate in precedenza dagli scatterplot come le più discriminanti:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, c("Bottom", "Diagonal")]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
Anche in questo caso si nota che il valore di BIC è notevolmente più alto (migliore) rispetto al caso in cui sono state usate tutte le variabili. L'algoritmo suggerisce ancora un numero ottimale di cluster pari a 3, ma questa volta ha selezionato come modello parsimonioso il modello EVE (ellissoidale, con volume uguale e orientamento variabile). È importante osservare che in uno dei cluster sono presenti solamente 16 osservazioni, il che potrebbe indicare un gruppo di dati che, probabilmente, sono state erroneamente assegnate a un cluster separato.  
Verifichiamo ora se i valori di ICL sono in accordo con quelli del BIC confrontando esplicitamente le soluzioni per 2 e 3 cluster:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, c("Bottom", "Diagonal")]
dati_scaled <- scale(dati)
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```
In questo caso, i valori di BIC e ICL sono concordi: per un numero di cluster pari a 3 si ottiene un BIC di circa -845 e un ICL di -849, mentre per 2 cluster il BIC risulta attorno a -888 e l'ICL a -890. Questo dimostra che anche l'ICL, che penalizza ulteriormente le assegnazioni incerte, suggerisce che il modello ottimale è quello con 3 cluster.  
Adesso, procederemo ad utilizzare solamente le variabili estratte tramite la tecnica PCA.  


```{r}
dati <- banknote_pca[, -1]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
In questo caso si nota che il valore di BIC è più alto (migliore) rispetto al caso in cui sono state usate tutte le variabili ma è più basso del caso in cui sono state usate solamente le due variabili selezionate. Questa volta l'algoritmo suggerisce un numero ottimale di cluster pari a 2, selezionando come modello parsimonioso il modello EEV (ellissoidale, con volume e orientamento uguale).  
Verifichiamo ora se i valori di ICL sono in accordo con quelli del BIC confrontando esplicitamente le soluzioni per 2 e 3 cluster:  

```{r}
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```

Anche in questo caso, i valori di BIC e ICL sono concordi: per un numero di cluster pari a 3 si ottiene un BIC di circa -1630 e un ICL di -1657, mentre per 2 cluster il BIC risulta attorno a -1614 e l'ICL a -1616. Questo dimostra che anche l'ICL suggerisce che il modello ottimale è quello con 2 cluster.  


# 6. Validazione e visualizzazione dei risultati

TODO: inserire commento introduttivo

## 6.3. Validazione con la classe reale

In questa sezione analizzeremo la qualità dei diversi metodi di clustering applicati al dataset banknote, fissando il numero di cluster pari al numero di classi reali, ovvero 2. Questo approccio ci permette di utilizzare tecniche di validazione esterne, dato che disponiamo delle etichette reali delle osservazioni (variabile 'Status').  
Per valutare l'efficacia dei cluster identificati, utilizzeremo diverse metriche, tra cui:  
- Adjusted Rand Index (ARI) e Meila Variation Index, che misurano il grado di concordanza tra il clustering ottenuto e la classificazione reale.
- Metriche di valutazione proprie del machine learning supervisionato, come accuratezza, specificità, recall, matrice di confusione e AUC, per interpretare il clustering come un problema di classificazione binaria.  
Verranno valutate le misure di validazione al variare del numero di variabili utilizzate nel clustering. In particolare, analizzeremo tre configurazioni differenti:  
- **Dataset completo**: utilizzo di tutte le variabili disponibili.
- **Dataset ridotto**: utilizzo esclusivo delle variabili 'Diagonal' e 'Bottom', identificate come le più discriminative.
- **Dataset trasformato**: utilizzo delle variabili ottenute tramite PCA.

### 6.3.1. Dataset completo 

Nel seguente codice, applichiamo i diversi algoritmi di clustering al dataset originale con i migliori parametri calcolati in precedenza (fissando il numero di cluster a 2) e aggiungiamo al dataframe le colonne corrispondenti alle assegnazioni di ciascun metodo. Da precisare che in aggiunta agli algoritmi discussi in precedenza, confrontiamo anche l'algoritmo gerarchico divisivo che non è stato affrontato prima.

```{r}
library(cluster) 
data("banknote")
data <- banknote[, -1]
data_scaled <- scale(data)

set.seed(123)
km_model <- kmeans(data_scaled, centers = 2, nstart = 25)
pam_model <- pam(data_scaled, k = 2, nstart = 25)
hc_model <- hclust(dist(data_scaled, method = "euclidean"), method = "ward.D2")
diana_model <- diana(data_scaled)
gmm_model <- Mclust(data_scaled, G = 2)

banknote$kmeans <- km_model$cluster
banknote$kmedoids <- pam_model$clustering
banknote$hierarchical_agglomerative <- cutree(hc_model, k = 2)
banknote$hierarchical_divisive <- cutree(as.hclust(diana_model), k = 2)
banknote$gmm <- gmm_model$classification

head(banknote)
```

Il codice successivo implementa una funzione per riallineare le assegnazioni dei cluster alle classi reali in modo da rendere i confronti più accurati.

```{r}
library(caret)
library(pROC)
library(mclust)

bestMapping <- function(true, pred) {

  banknote <- banknote %>%
  mutate(Status_numeric = recode(Status,
                                 "counterfeit" = 1,
                                 "genuine" = 2))


  true <- banknote$Status_numeric
  true <- as.character(true)
  pred <- as.character(pred)
  levels_true <- sort(unique(true))
  
  cm1 <- confusionMatrix(factor(pred, levels = levels_true), factor(true, levels = levels_true))
  acc1 <- cm1$overall["Accuracy"]
  
  pred_swap <- ifelse(pred == levels_true[1], levels_true[2], levels_true[1])
  cm2 <- confusionMatrix(factor(pred_swap, levels = levels_true), factor(true, levels = levels_true))
  acc2 <- cm2$overall["Accuracy"]
  
  if(acc2 > acc1) {
    return(list(mapped = pred_swap, cm = cm2))
  } else {
    return(list(mapped = pred, cm = cm1))
  }
}

```

Nel blocco successivo, calcoliamo le metriche di valutazione per ciascun algoritmo di clustering. È importante notare che il valore di AUC ha un significato più chiaro per l'algoritmo GMM (Gaussian Mixture Model), poiché questo metodo fornisce direttamente le probabilità a posteriori di appartenenza ai cluster. Per gli altri algoritmi, che operano in modo hard (assegnando ciascun punto a un solo cluster senza una misura di incertezza), il calcolo dell'AUC è stato forzato attraverso:

- Per K-means e K-medoids, è stato definito uno score basato sulla distanza dei punti dal centroide o dal medoide del rispettivo cluster.  
- Per gli algoritmi gerarchici, non è stato possibile adottare un criterio analogo per costruire uno score continuo, motivo per cui l'AUC non è stato calcolato (indicata come NA nella tabella).  

```{r}
# K-means
ari_kmeans   <- adjustedRandIndex(banknote$Status, banknote$kmeans)
mapping_kmeans <- bestMapping(banknote$Status, banknote$kmeans)
cm_kmeans <- mapping_kmeans$cm
acc_kmeans   <- cm_kmeans$overall["Accuracy"]
sens_kmeans  <- cm_kmeans$byClass["Sensitivity"]
spec_kmeans  <- cm_kmeans$byClass["Specificity"]

distances_km <- t(apply(data_scaled, 1, function(x) {
  apply(km_model$centers, 1, function(cent) sqrt(sum((x - cent)^2)))
}))
score_kmeans <- distances_km[,2] / rowSums(distances_km)
roc_obj_kmeans <- roc(banknote$Status, score_kmeans)
auc_kmeans <- auc(roc_obj_kmeans)

# K-medoids
ari_kmedoids <- adjustedRandIndex(banknote$Status, banknote$kmedoids)
mapping_kmedoids <- bestMapping(banknote$Status, banknote$kmedoids)
cm_kmedoids <- mapping_kmedoids$cm
acc_kmedoids   <- cm_kmedoids$overall["Accuracy"]
sens_kmedoids  <- cm_kmedoids$byClass["Sensitivity"]
spec_kmedoids  <- cm_kmedoids$byClass["Specificity"]

medoids <- as.matrix(pam_model$medoids)
distances_kmed <- t(apply(data_scaled, 1, function(x) {
  apply(medoids, 1, function(med) sqrt(sum((x - med)^2)))
}))
score_kmedoids <- distances_kmed[,2] / rowSums(distances_kmed)
roc_obj_kmedoids <- roc(banknote$Status, score_kmedoids)
auc_kmedoids <- auc(roc_obj_kmedoids)

# Gerarchico Agglomerativo
ari_hierAgg  <- adjustedRandIndex(banknote$Status, banknote$hierarchical_agglomerative)
mapping_hierAgg <- bestMapping(banknote$Status, banknote$hierarchical_agglomerative)
cm_hierAgg <- mapping_hierAgg$cm
acc_hierAgg   <- cm_hierAgg$overall["Accuracy"]
sens_hierAgg  <- cm_hierAgg$byClass["Sensitivity"]
spec_hierAgg  <- cm_hierAgg$byClass["Specificity"]
auc_hierAgg <- NA

# Gerarchico Divisivo
ari_hierDiv  <- adjustedRandIndex(banknote$Status, banknote$hierarchical_divisive)
mapping_hierDiv <- bestMapping(banknote$Status, banknote$hierarchical_divisive)
cm_hierDiv <- mapping_hierDiv$cm
acc_hierDiv   <- cm_hierDiv$overall["Accuracy"]
sens_hierDiv  <- cm_hierDiv$byClass["Sensitivity"]
spec_hierDiv  <- cm_hierDiv$byClass["Specificity"]
auc_hierDiv <- NA

# GMM
ari_gmm <- adjustedRandIndex(banknote$Status, banknote$gmm)
mapping_gmm <- bestMapping(banknote$Status, banknote$gmm)
cm_gmm <- mapping_gmm$cm
acc_gmm   <- cm_gmm$overall["Accuracy"]
sens_gmm  <- cm_gmm$byClass["Sensitivity"]
spec_gmm  <- cm_gmm$byClass["Specificity"]
score_gmm <- gmm_model$z[,2]
roc_obj_gmm <- roc(banknote$Status, score_gmm)
auc_gmm <- auc(roc_obj_gmm)

```
Infine, raccogliamo i risultati in un dataframe riassuntivo.

```{r}
results_df <- data.frame(
  Metodo       = c("K-means", "K-medoids", "Gerarchico Agglomerativo", "Gerarchico Divisivo", "GMM"),
  ARI = c(ari_kmeans, ari_kmedoids, ari_hierAgg, ari_hierDiv, ari_gmm),
  Accuracy     = c(acc_kmeans, acc_kmedoids, acc_hierAgg, acc_hierDiv, acc_gmm),
  Sensitivity  = c(sens_kmeans, sens_kmedoids, sens_hierAgg, sens_hierDiv, sens_gmm),
  Specificity  = c(spec_kmeans, spec_kmedoids, spec_hierAgg, spec_hierDiv, spec_gmm),
  AUC          = c(auc_kmeans, auc_kmedoids, auc_hierAgg, auc_hierDiv, auc_gmm)
)
print(results_df)
```

I risultati mostrano che il metodo **GMM** ha la migliore performance complessiva, con il più alto valore di ARI (0.98) e un'accuratezza del 99.5%. Il clustering gerarchico agglomerativo ha anch'esso buone prestazioni, mentre K-means e K-medoids mostrano una buona affidabilità ma con AUC leggermente inferiori. Analizzando la sensibilità e la specificità, si nota che la sensibilità – ovvero la capacità di identificare correttamente le banconote contraffatte – è generalmente molto alta per tutti gli algoritmi, indicando un'elevata capacità di individuare i falsi. Tuttavia, la specificità, ossia la capacità di riconoscere correttamente le banconote genuine, varia maggiormente tra i metodi. In particolare, K-means mostra una specificità inferiore (0.92) rispetto a K-medoids (0.97) e ai metodi gerarchici (0.98), suggerendo una maggiore tendenza a classificare erroneamente alcune banconote genuine come contraffatte; questo errore è meno grave di classificare una banconota contraffatta come vera.

### 6.3.2. Dataset ridotto

Nel seguente codice, applichiamo i diversi algoritmi di clustering al dataset ridotto (con le sole variabili 'Diagonal' e 'Bottom') con i migliori parametri calcolati in precedenza (fissando il numero di cluster a 2) e aggiungiamo al dataframe le colonne corrispondenti alle assegnazioni di ciascun metodo.

```{r}
library(cluster) 
data("banknote")
data <- banknote[, c("Bottom", "Diagonal")]
data_scaled <- scale(data)

set.seed(123)
km_model <- kmeans(data_scaled, centers = 2, nstart = 25)
pam_model <- pam(data_scaled, k = 2, nstart = 25)
#TODO: verificare se ward.D2 è il migliore
hc_model <- hclust(dist(data_scaled, method = "euclidean"), method = "ward.D2")
diana_model <- diana(data_scaled)
gmm_model <- Mclust(data_scaled, G = 2)

banknote$kmeans <- km_model$cluster
banknote$kmedoids <- pam_model$clustering
banknote$hierarchical_agglomerative <- cutree(hc_model, k = 2)
banknote$hierarchical_divisive <- cutree(as.hclust(diana_model), k = 2)
banknote$gmm <- gmm_model$classification

head(banknote)
```

Nel blocco successivo, calcoliamo le metriche di valutazione come nel caso precedente.

```{r}
# K-means
ari_kmeans   <- adjustedRandIndex(banknote$Status, banknote$kmeans)
mapping_kmeans <- bestMapping(banknote$Status, banknote$kmeans)
cm_kmeans <- mapping_kmeans$cm
acc_kmeans   <- cm_kmeans$overall["Accuracy"]
sens_kmeans  <- cm_kmeans$byClass["Sensitivity"]
spec_kmeans  <- cm_kmeans$byClass["Specificity"]

distances_km <- t(apply(data_scaled, 1, function(x) {
  apply(km_model$centers, 1, function(cent) sqrt(sum((x - cent)^2)))
}))
score_kmeans <- distances_km[,2] / rowSums(distances_km)
roc_obj_kmeans <- roc(banknote$Status, score_kmeans)
auc_kmeans <- auc(roc_obj_kmeans)

# K-medoids
ari_kmedoids <- adjustedRandIndex(banknote$Status, banknote$kmedoids)
mapping_kmedoids <- bestMapping(banknote$Status, banknote$kmedoids)
cm_kmedoids <- mapping_kmedoids$cm
acc_kmedoids   <- cm_kmedoids$overall["Accuracy"]
sens_kmedoids  <- cm_kmedoids$byClass["Sensitivity"]
spec_kmedoids  <- cm_kmedoids$byClass["Specificity"]

medoids <- as.matrix(pam_model$medoids)
distances_kmed <- t(apply(data_scaled, 1, function(x) {
  apply(medoids, 1, function(med) sqrt(sum((x - med)^2)))
}))
score_kmedoids <- distances_kmed[,2] / rowSums(distances_kmed)
roc_obj_kmedoids <- roc(banknote$Status, score_kmedoids)
auc_kmedoids <- auc(roc_obj_kmedoids)

# Gerarchico Agglomerativo
ari_hierAgg  <- adjustedRandIndex(banknote$Status, banknote$hierarchical_agglomerative)
mapping_hierAgg <- bestMapping(banknote$Status, banknote$hierarchical_agglomerative)
cm_hierAgg <- mapping_hierAgg$cm
acc_hierAgg   <- cm_hierAgg$overall["Accuracy"]
sens_hierAgg  <- cm_hierAgg$byClass["Sensitivity"]
spec_hierAgg  <- cm_hierAgg$byClass["Specificity"]
auc_hierAgg <- NA

# Gerarchico Divisivo
ari_hierDiv  <- adjustedRandIndex(banknote$Status, banknote$hierarchical_divisive)
mapping_hierDiv <- bestMapping(banknote$Status, banknote$hierarchical_divisive)
cm_hierDiv <- mapping_hierDiv$cm
acc_hierDiv   <- cm_hierDiv$overall["Accuracy"]
sens_hierDiv  <- cm_hierDiv$byClass["Sensitivity"]
spec_hierDiv  <- cm_hierDiv$byClass["Specificity"]
auc_hierDiv <- NA

# GMM
ari_gmm <- adjustedRandIndex(banknote$Status, banknote$gmm)
mapping_gmm <- bestMapping(banknote$Status, banknote$gmm)
cm_gmm <- mapping_gmm$cm
acc_gmm   <- cm_gmm$overall["Accuracy"]
sens_gmm  <- cm_gmm$byClass["Sensitivity"]
spec_gmm  <- cm_gmm$byClass["Specificity"]
score_gmm <- gmm_model$z[,2]
roc_obj_gmm <- roc(banknote$Status, score_gmm)
auc_gmm <- auc(roc_obj_gmm)

```

Infine, raccogliamo i risultati in un dataframe riassuntivo.

```{r}
results_df <- data.frame(
  Metodo       = c("K-means", "K-medoids", "Gerarchico Agglomerativo", "Gerarchico Divisivo", "GMM"),
  ARI = c(ari_kmeans, ari_kmedoids, ari_hierAgg, ari_hierDiv, ari_gmm),
  Accuracy     = c(acc_kmeans, acc_kmedoids, acc_hierAgg, acc_hierDiv, acc_gmm),
  Sensitivity  = c(sens_kmeans, sens_kmedoids, sens_hierAgg, sens_hierDiv, sens_gmm),
  Specificity  = c(spec_kmeans, spec_kmedoids, spec_hierAgg, spec_hierDiv, spec_gmm),
  AUC          = c(auc_kmeans, auc_kmedoids, auc_hierAgg, auc_hierDiv, auc_gmm)
)
print(results_df)
```

Questa configurazione semplifica il modello senza perdere accuratezza, dimostrando che 'Bottom' e 'Diagonal' sono le variabili chiave per distinguere le classi:  
- K-means, K-medoids e GMM ottengono ARI 0.98, accuracy 99.5% e AUC ~1, confermando che queste due variabili sono altamente discriminative.  
- GMM raggiunge sensibilità 100%, identificando tutte le banconote contraffatte, mentre K-means e K-medoids garantiscono specificità 100%, evitando falsi positivi.  

### 6.3.3. Dataset trasformato

Nel seguente codice, applichiamo i diversi algoritmi di clustering al dataset trasformato (con le variabili trasformate mediante tecnica PCA) con i migliori parametri calcolati in precedenza (fissando il numero di cluster a 2) e aggiungiamo al dataframe le colonne corrispondenti alle assegnazioni di ciascun metodo.

```{r}
data <- banknote_pca[, -1]
data_scaled <- scale(data)

set.seed(123)
km_model <- kmeans(data_scaled, centers = 2, nstart = 25)
pam_model <- pam(data_scaled, k = 2, nstart = 25)
#TODO: verificare se ward.D2 è il migliore
hc_model <- hclust(dist(data_scaled, method = "euclidean"), method = "ward.D2")
diana_model <- diana(data_scaled)
gmm_model <- Mclust(data_scaled, G = 2)

banknote$kmeans <- km_model$cluster
banknote$kmedoids <- pam_model$clustering
banknote$hierarchical_agglomerative <- cutree(hc_model, k = 2)
banknote$hierarchical_divisive <- cutree(as.hclust(diana_model), k = 2)
banknote$gmm <- gmm_model$classification

head(banknote)
```

Nel blocco successivo, calcoliamo le metriche di valutazione come nei casi precedenti.

```{r}
# K-means
ari_kmeans   <- adjustedRandIndex(banknote$Status, banknote$kmeans)
mapping_kmeans <- bestMapping(banknote$Status, banknote$kmeans)
cm_kmeans <- mapping_kmeans$cm
acc_kmeans   <- cm_kmeans$overall["Accuracy"]
sens_kmeans  <- cm_kmeans$byClass["Sensitivity"]
spec_kmeans  <- cm_kmeans$byClass["Specificity"]

distances_km <- t(apply(data_scaled, 1, function(x) {
  apply(km_model$centers, 1, function(cent) sqrt(sum((x - cent)^2)))
}))
score_kmeans <- distances_km[,2] / rowSums(distances_km)
roc_obj_kmeans <- roc(banknote$Status, score_kmeans)
auc_kmeans <- auc(roc_obj_kmeans)

# K-medoids
ari_kmedoids <- adjustedRandIndex(banknote$Status, banknote$kmedoids)
mapping_kmedoids <- bestMapping(banknote$Status, banknote$kmedoids)
cm_kmedoids <- mapping_kmedoids$cm
acc_kmedoids   <- cm_kmedoids$overall["Accuracy"]
sens_kmedoids  <- cm_kmedoids$byClass["Sensitivity"]
spec_kmedoids  <- cm_kmedoids$byClass["Specificity"]

medoids <- as.matrix(pam_model$medoids)
distances_kmed <- t(apply(data_scaled, 1, function(x) {
  apply(medoids, 1, function(med) sqrt(sum((x - med)^2)))
}))
score_kmedoids <- distances_kmed[,2] / rowSums(distances_kmed)
roc_obj_kmedoids <- roc(banknote$Status, score_kmedoids)
auc_kmedoids <- auc(roc_obj_kmedoids)

# Gerarchico Agglomerativo
ari_hierAgg  <- adjustedRandIndex(banknote$Status, banknote$hierarchical_agglomerative)
mapping_hierAgg <- bestMapping(banknote$Status, banknote$hierarchical_agglomerative)
cm_hierAgg <- mapping_hierAgg$cm
acc_hierAgg   <- cm_hierAgg$overall["Accuracy"]
sens_hierAgg  <- cm_hierAgg$byClass["Sensitivity"]
spec_hierAgg  <- cm_hierAgg$byClass["Specificity"]
auc_hierAgg <- NA

# Gerarchico Divisivo
ari_hierDiv  <- adjustedRandIndex(banknote$Status, banknote$hierarchical_divisive)
mapping_hierDiv <- bestMapping(banknote$Status, banknote$hierarchical_divisive)
cm_hierDiv <- mapping_hierDiv$cm
acc_hierDiv   <- cm_hierDiv$overall["Accuracy"]
sens_hierDiv  <- cm_hierDiv$byClass["Sensitivity"]
spec_hierDiv  <- cm_hierDiv$byClass["Specificity"]
auc_hierDiv <- NA

# GMM
ari_gmm <- adjustedRandIndex(banknote$Status, banknote$gmm)
mapping_gmm <- bestMapping(banknote$Status, banknote$gmm)
cm_gmm <- mapping_gmm$cm
acc_gmm   <- cm_gmm$overall["Accuracy"]
sens_gmm  <- cm_gmm$byClass["Sensitivity"]
spec_gmm  <- cm_gmm$byClass["Specificity"]
score_gmm <- gmm_model$z[,2]
roc_obj_gmm <- roc(banknote$Status, score_gmm)
auc_gmm <- auc(roc_obj_gmm)

```

Infine, raccogliamo i risultati in un dataframe riassuntivo.

```{r}
results_df <- data.frame(
  Metodo       = c("K-means", "K-medoids", "Gerarchico Agglomerativo", "Gerarchico Divisivo", "GMM"),
  ARI = c(ari_kmeans, ari_kmedoids, ari_hierAgg, ari_hierDiv, ari_gmm),
  Accuracy     = c(acc_kmeans, acc_kmedoids, acc_hierAgg, acc_hierDiv, acc_gmm),
  Sensitivity  = c(sens_kmeans, sens_kmedoids, sens_hierAgg, sens_hierDiv, sens_gmm),
  Specificity  = c(spec_kmeans, spec_kmedoids, spec_hierAgg, spec_hierDiv, spec_gmm),
  AUC          = c(auc_kmeans, auc_kmedoids, auc_hierAgg, auc_hierDiv, auc_gmm)
)
print(results_df)
```

Rispetto ai risultati con il dataset completo o ridotto, l'uso di solo 3 variabili ottenute con PCA ha ridotto la performance globale, in particolare per K-medoids e Gerarchico Agglomerativo. Sebbene l'accuratezza resti elevata, l'ARI e la specificità mostrano una leggera diminuzione, il che suggerisce che la riduzione delle variabili ha compromesso la capacità di questi algoritmi di catturare la struttura dei dati. In particolare, l'algoritmo Gerarchico Agglomerativo mostra una performance molto più bassa in termini di ARI e accuratezza, indicando che l'algoritmo è più sensibile alla perdita di variabili discriminanti.  

In conclusione, dal punto di vista della misurazione delle performance con tecniche esterne (avendo a disposizione la vera classe delle osservazioni), l'algoritmo **GMM** è risultato il migliore nei tre diversi dataset, anche se, tutto sommato, gli altri algoritmi non hanno ottenuto dei cattivi risultati.

# 7. Conclusioni

Questo progetto, realizzato a scopo didattico, ha analizzato diverse tecniche di clustering applicate al dataset *banknote*, un dataset semplice con due classi ben distinte. Inizialmente sono state utilizzate misure interne – quali il BIC, l'indice silhouette e altre metriche di compattezza – per valutare la qualità dei cluster senza ricorrere alle etichette reali. Successivamente, l'analisi è stata estesa con misure esterne (Adjusted Rand Index, accuratezza, sensibilità, specificità e AUC) per confrontare le assegnazioni dei cluster con la vera classe delle osservazioni.

I risultati mostrano che, quando si utilizzano tutte le variabili o solo quelle discriminanti (Bottom e Diagonal), algoritmi come GMM e, in alcuni casi, K-means e K-medoids raggiungono prestazioni eccellenti, con ARI e AUC quasi perfetti. Al contrario, i metodi gerarchici, in particolare quello agglomerativo, hanno ottenuto risultati leggermente inferiori, anche se in alcune configurazioni il clustering gerarchico divisivo si è dimostrato competitivo. L'approccio GMM risulta particolarmente robusto, grazie alla possibilità di estrarre score continui tramite le probabilità a posteriori.

Questi risultati, sia interni che esterni, evidenziano come una corretta selezione o trasformazione delle variabili (ad es. tramite PCA) possa semplificare il modello senza compromettere la capacità di distinguere correttamente le banconote genuine da quelle contraffatte.

Per il futuro, un possibile sviluppo potrebbe essere l'applicazione di questi metodi a dataset più complessi, nonché l'ottimizzazione degli score per metodi hard. Inoltre, integrare tecniche di soft clustering o combinare il clustering con approcci supervisionati potrebbe migliorare ulteriormente la generalizzazione e la robustezza dei modelli, offrendo una visione più completa delle performance in contesti reali.




