---
title: "Analisi di Clustering sul Dataset Banknote"
author: "D'Atri, Lombardi, Maiello"
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

packages <- c("mclust", "reshape2", "ggplot2", "GGally", "dplyr", "clustertend", "cluster", "FNN", "FactoMineR", "factoextra", "caret", "pheatmap", "clusterCrit", "NbClust", "fpc")

for (pkg in packages) {
    if (!requireNamespace(pkg, quietly = TRUE)) {
        install.packages(pkg, repos = "https://cran.rstudio.com/")
    }
}
```

# 1. Introduzione

## Obiettivo del Progetto

Il presente progetto si propone di applicare diverse tecniche di clustering al dataset **banknote** al fine di identificare gruppi omogenei presenti nei dati e confrontare l'efficacia dei vari metodi. L'analisi intende mettere in pratica le metodologie apprese nel modulo di *Data Analysis & Statistical Learning*, in particolare, verranno trattati i seguenti algoritmi di clustering: **Clustering Gerarchico Agglomerativo**, **Clustering Partizionale (K-means e K-medoids)** e **Clustering Model-based (Mixture of Gaussian)**.

TODO: specificare PCA e mixture of regression

Tra diversi dataset consigliati dai docenti, è stato scelto il dataset **banknote**, per un motivo puramente professionale, essendo tutti i membri del team legati professionalmente alle banconote.
TODO: togliere?

Il documento è organizzato nei seguenti capitoli:

1.  **Introduzione**: Presentazione del progetto, degli obiettivi e del contesto.
2.  **Descrizione del Dataset e Analisi Esplorativa**: Esplorazione preliminare dei dati, analisi descrittiva e operazioni di preprocessing.
3.  **Applicazione delle Tecniche di Clustering**: Implementazione dei metodi di clustering (gerarchico, partizionale e model-based).
4.  **Validazione e Visualizzazione dei Risultati**: Confronto dei risultati mediante metriche di validazione e visualizzazione dei cluster (con l'ausilio della PCA).
5.  **Conclusioni e Sviluppi Futuri**: Sintesi dei risultati ottenuti e prospettive per ulteriori analisi.


# 2. Descrizione del Dataset e Analisi Esplorativa

In questa sezione esamineremo il dataset **banknote** per comprenderne la struttura, le caratteristiche e le relazioni tra le variabili. Il dataset proviene dal pacchetto `mclust` e contiene misure che possono essere utilizzate per distinguere tra banconote autentiche e false (o per altri scopi diagnostici).
Il dataset contiene le seguenti variabili:

- Status: lo stato della banconota (autentica o contraffatta)

- Length: lunghezza della banconota (mm)

- Left: larghezza del bordo sinistro (mm)

- Right: larghezza del bordo destro (mm)

- Bottom: larghezza del margine inferiore (mm)

- Top: larghezza del margine superiore (mm)

- Diagonal: lunghezza della diagonale (mm)

## Caricamento del Dataset

```{r load-data}
library(mclust)
data(banknote)
```

## Esplorazione della Struttura e Statistiche Descrittive del dataset
Visualizziamo le prime righe del dataset

```{r head}
head(banknote)
```

Esaminiamo la struttura del dataset
```{r str}
str(banknote)
```
Il dataset è composto da 200 osservazioni e 7 variabili, tutte di tipo numerico ad eccezione di 'Status' che è una variabile binaria che può assumere uno tra i seguenti valori: 'counterfeit' e 'genuine'.

Visualizziamo un riepilogo statistico delle variabili.

```{r summary}
summary(banknote)
```
Notiamo che la colonna 'Status' è perfettamente bilanciata.

# Controllo valori mancanti
```{r missing values}
colSums(is.na(banknote))
```
Non c'è nessun valore mancante nelle colonne.

## Analisi Esplorativa dei Dati (EDA)
Questa sezione fornisce una panoramica iniziale del dataset banknote, evidenziando la struttura, le principali statistiche descrittive e le relazioni tra le variabili.
Queste informazioni sono fondamentali per orientare le successive fasi di clustering e per capire eventuali necessità di preprocessing (ad es. scaling o trasformazioni) prima
dell'applicazione dei metodi di clustering.

### Analisi delle Correlazioni

Verifichiamo le correlazioni tra le variabili numeriche per capire se esistono relazioni forti che potrebbero influenzare l'analisi di clustering.

```{r}
cor_matrix <- cor(banknote[, -1])
print(cor_matrix)
library(reshape2)
library(ggplot2)

cor_melted <- melt(cor_matrix)
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) + geom_tile() +
geom_text(aes(label = round(value, 2)), color = "black",
size = 5) + scale_fill_gradient2(low = "blue", mid = "white",
high = "red", midpoint = 0, limits = c(-1, 1)) + theme_minimal() +
labs(title = "Correlazione variabili", fill = "Correlazione") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

TODO: Commento:

```
Osserva i valori di correlazione per identificare variabili fortemente correlate (positivamente o negativamente).

Discuti l’impatto di eventuali correlazioni elevate: potrebbero ridondare in informazioni e influenzare il clustering.

Se trovi correlazioni elevate, suggerisci che una riduzione della dimensionalità (es. tramite PCA) potrebbe aiutare.
```

### Distribuzione delle Variabili

Infine, esaminiamo la distribuzione di ciascuna variabile con degli istogrammi:

```{r}
# Impostiamo un layout per visualizzare gli istogrammi
par(mfrow = c(2, 2))
for(i in 2:7){
    hist(banknote[[i]], main = paste("Istogramma di", names(banknote)[i]),
    xlab = names(banknote)[i], col = "lightblue", border = "white")
}
par(mfrow = c(1, 1))  # Ripristiniamo il layout originale

```

TODO: Commento:

```
Descrivi la forma delle distribuzioni: simmetriche, asimmetriche (skewed), unimodali o multimodali.

Nota eventuali outlier o code lunghe che potrebbero influenzare la successiva analisi di clustering.

Confronta le distribuzioni tra le variabili per capire se alcune variabili hanno una variabilità molto diversa, che potrebbe richiedere una normalizzazione/scaling.
```

### Matrice di Scatterplot

Per avere una prima idea delle relazioni tra le variabili, creiamo una matrice di scatterplot. Utilizziamo la colonna 'Status' per colorare i punti.

```{r scatterplot}
library(GGally)
ggpairs(banknote[,-1],upper = list(continuous = "density", combo = "box_no_facet"),
        lower = list(continuous = "points", combo = "dot_no_facet"),aes(colour=banknote$Status))
```

Lungo la diagonale sono visibili le distribuzioni univariate delle variabili. Si nota una sovrapposizione parziale tra i due gruppi di colore (Status), ma anche zone in cui uno dei due colori prevale.
Nei grafici di densità bivariata (metà diagonale superiore della matrice) e negli scatter plot (metà diagonale inferiore della matrice), si osservano alcune regioni in cui i punti di uno stesso colore tendono a concentrarsi, suggerendo cluster parzialmente distinti.
Nonostante la sovrapposizione in diverse aree, si intravedono regioni in cui un gruppo prevale. Ciò suggerisce che un algoritmo di classificazione o di clustering potrebbe distinguere parzialmente le due classi, anche se non in modo perfetto con una singola coppia di variabili.
È importante sottolineare che questo grafico mostra solo le relazioni a coppie. Ciò significa che l'analisi si limita a considerare due variabili alla volta, mentre una clusterizzazione netta potrebbe emergere solo quando si considerano contemporaneamente più variabili. In altre parole, anche se alcune coppie non evidenziano una separazione chiara, combinando più dimensioni si potrebbe scoprire una struttura clusterizzata più marcata.
Il fatto che in alcune proiezioni (coppie di variabili) si vedano zone di separazione o contorni distinti suggerisce che esistono sottostrutture nei dati.


## Metodi per valutare la tendenza al clustering del dataset banknote
Misure che aiutano a determinare se un dataset è adatto per il clustering, valutando quanto i dati siano distribuiti in modo casuale o contengano una struttura raggruppata.

### Hopkins statistics
La Hopkins statistic serve a valutare la tendenza al clustering del dataset reale confrontando la sua struttura con quella di un insieme di punti generati casualmente.
Si tratta di una una misura che può assumere valori tra 0 e 1, più il valore è vicino allo 0 più il dataset ha una struttura clusterizzata (ossia presenta una buona separazione tra gruppi).
Se assume un valore tra 0.5 o più significa che il dataset non presenta una struttura di gruppo significativa e probabilmente le osservazione sono distribuite con una distribuzione uniforme.
```{r}
library(clustertend)

banknote_num <- banknote[, -1]

banknote_num <- scale(banknote_num)

set.seed(987)

hopkins_stat <- hopkins(banknote_num, n = nrow(banknote_num) - 1)

random_df <- apply(banknote_num, 2,
                   function(x){runif(length(x), min(x), max(x))})
random_df <- as.data.frame(random_df)

random_df <- scale(random_df)

set.seed(987)

hopkins_stat_rand <- hopkins(random_df, n = nrow(random_df)-1)

cat("Hopkins Statistics on Banknote dataset:", hopkins_stat$H, "\n")
cat("Hopkins Statistics on Random dataset:", hopkins_stat_rand$H, "\n")
```
Un valore di Hopkins pari a 0.2519264, essendo inferiore a 0.5, indica una tendenza alla clusterizzazione. In altre parole, le distanze tra i punti reali sono molto più ridotte rispetto a quelle dei punti generati casualmente, suggerendo la presenza di gruppi ben definiti nel dataset. Questo risultato supporta l'ipotesi che il dataset sia strutturato in cluster distinti.

### Visual Assessment of cluster Tendency (VAT)
L'algoritmo VAT produce una matrice di dissimilarità ordinata, ottenuta riorganizzando le righe e le colonne della matrice delle distanze per evidenziare eventuali blocchi di punti “ravvicinati” (ossia potenziali cluster). Visualizziamo la matrice con l'intento di verificare se si formano lungo la diagonale secondaria del grafico, dei blocchi rossi che rappresentano regioni di osservazioni che condividono una forte similarità.

```{r}
library("factoextra")

fviz_dist(dist(banknote), show_labels = FALSE) +
labs(title = "Banknote data")

fviz_dist(dist(random_df), show_labels = FALSE) +
labs(title = "Random data")
```

Nei grafici VAT il rosso indica alta similarità (ossia bassa dissimilarità), mentre il blu indica bassa similarità (ossia alta dissimilarità).
Relativamente al grafico del dataset banknote, notiamo che lungo la diagonale sono presenti regioni rosse che indicano dati simili tra loro, suggerendo la presenza di cluster.
La presenza di zone blu che evidenziano una maggiore dissimilarità tra gruppi di dati, supporta l'ipotesi che esistano cluster distinti nel dataset.
Inoltre la presenza di transizioni marcate dal rosso al blu conferma che esistono confini ben definiti tra i cluster.

## PCA
In questa sezione applichiamo la PCA sul dataset, considerando solo le colonne numeriche. Anche se le variabili sono tutte misurate in millimetri e quindi nella stessa scala, applichiamo comunque la standardizzazione per evitare che differenze nella dispersione (variabili con varianze maggiori) influenzino in modo sproporzionato i risultati della PCA.
```{r}
library("FactoMineR")

banknote_num <- banknote[, -1]
head(banknote_num)
pca_result <- PCA(banknote_num, graph = FALSE)
```

#### Visualizzazione e interpretazione dei risultati
Estraiamo gli autovalori e calcoliamo sia la varianza spiegata per ciascuna componente che la varianza cumulativa. Queste informazioni ci serviranno per decidere il numero ottimale di componenti da mantenere.
```{r}
eigenvalues <- get_eigenvalue(pca_result)[,1]
explained_variance <- eigenvalues / sum(eigenvalues)
cumulative_variance <- cumsum(explained_variance)

print(eigenvalues)
print(explained_variance)
print(cumulative_variance)
```
Abbiamo ora a disposizione gli autovalori e le informazioni sulla varianza, utili per valutare l'importanza di ciascuna componente principale.

Utilizziamo la Kaiser Rule per selezionare le componenti con autovalori superiori a 1.
```{r kaiser rule}
kaiser_components <- sum(eigenvalues > 1)
cat("Numero di componenti selezionati con la Kaiser rule:", kaiser_components, "\n")
```
Un ulteriore criterio è quello di selezionare il numero minimo di componenti che, sommando la varianza spiegata, raggiungono almeno l'80% del totale. In questo modo garantiamo una buona rappresentazione dei dati.
```{r threshold}
variance_threshold <- min(which(cumulative_variance >= 0.80))

cat("Numero di componenti principali che spiegano l'80% della varianza comulativa:", variance_threshold, "\n")

fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))
```
Nel grafico vediamo la varianza spiegata da ciascuna componente. Con le prime tre componenti principali riusciamo a spiegare l'84.9% della varianza cumulativa.

Vediamo infine un ultimo metodo per scegliere il numero di componenti principali: il metodo dell'elbow.
Mediante un grafico scree plot visualizziamo gli autovalori per ciascuna componente e individuiamo "il gomito", ovvero il punto nel grafico in cui la diminuzione degli autovalori rallenta. Tale punto può essere interpretato come un'indicazione del numero ottimale di componenti. Includiamo nel grafico anche due linee verticali per evidenziare i criteri della varianza cumulativa (rosso) e della Kaiser Rule (blu).
```{r elbow}
scree_plot <- data.frame(PC = 1:length(eigenvalues), Eigenvalue = eigenvalues)

ggplot(scree_plot, aes(x = PC, y = Eigenvalue)) +
    geom_point() +
    geom_line() +
    geom_vline(xintercept = variance_threshold, linetype = "dashed", color = "red") +
    geom_vline(xintercept = kaiser_components, linetype = "dotted", color = "blue") +
    labs(title = "Scree Plot for PCA", x = "Principal Component", y = "Eigenvalue") +
    theme_minimal()
```

Nel grafico il gomito sembra collocarsi intorno alla quarta componente. I tre criteri danno perciò risultati differenti, per mantenere un equilibrio tra interpretabilità e preservazione della varianza, si potrebbe optare per 3 componenti.  
Di seguito viene costruito il nuovo dataset in cui le variabili sono sostituite con le prime 3 PCA, che verrà usato più avanti nelle analisi.

```{r}
pca_data <- as.data.frame(pca_result$ind$coord[, 1:3])
colnames(pca_data) <- c("PCA1", "PCA2", "PCA3")
banknote_pca <- cbind(pca_data, Status = banknote$Status)
banknote_pca <- banknote_pca %>% dplyr::select(Status, everything())
head(banknote_pca)
```


Per comprendere meglio il contributo delle variabili alle componenti principali, visualizziamo le loro coordinate e realizziamo una mappa di correlazione. Questo ci aiuta a capire come le variabili si relazionano alle componenti principali.
```{r}
var <- get_pca_var(pca_result)

var$coord

fviz_pca_var(pca_result, col.var = "contrib",
            gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
```
Questo passaggio ci offre una visione immediata di come ogni variabile contribuisce alle componenti principali e della loro interrelazione.

Approfondiamo l'analisi mostrando i contributi delle variabili attraverso diversi metodi grafici, in modo da identificare quali variabili influenzano maggiormente ciascuna componente.
```{r}
print(round(var$contrib,digits = 3))

library("corrplot")
corrplot(var$contrib, is.corr=FALSE)
```
L'analisi dei contributi evidenzia il peso relativo di ciascuna variabile nelle componenti principali, utile per interpretare i risultati della PCA.

Visualizziamo i contributi delle variabili specificamente per la prima componente principale.
```{r}
fviz_contrib(pca_result, choice = "var", axes = 1)
```
Similmente al passaggio precedente, visualizziamo i contributi delle variabili per la seconda componente principale.
```{r}
fviz_contrib(pca_result, choice = "var", axes = 2)
```

# 3. Applicazione delle Tecniche di Clustering

## 3.1 Clustering Gerarchico Agglomerativo

Il **Clustering Gerarchico Agglomerativo (HAC)** è una tecnica di clustering che costruisce una gerarchia di cluster attraverso un approccio "bottom-up". Inizia considerando ogni punto dati come un cluster separato e, successivamente, unisce iterativamente i cluster più
simili fino a formare un unico cluster che racchiude tutti i dati. Questo metodo non richiede la specifica del numero di cluster a priori e produce una rappresentazione ad albero, nota come **dendrogramma**, che facilita la visualizzazione delle relazioni tra i cluster.

Per applicare il clustering gerarchico agglomerativo al nostro dataset, seguiamo questi passaggi:

1.  **Scaling**:

2.  **Calcolo della Matrice di Distanza**: Determiniamo le distanze tra tutte le coppie di punti nel dataset. Una misura comune è la distanza euclidea.

3.  **Applicazione del clustering agglomerativo**: Utilizziamo la funzione `hclust()` per eseguire il clustering gerarchico e generare il dendrogramma.

4.  **Taglio del Dendrogramma**: Decidiamo il numero ottimale di cluster utilizzando varie tecniche, e tagliamo il dendrogramma al livello appropriato.

5.  **Valutazione**: Valutiamo i risultati del clustering utilizzando varie metriche.

### Clustering con tutte le variabili

Scaliamo il dataset, calcoliamo la relativa matrice di dissimilatità mediante distanza euclidea e la mostriamo.

```{r dissimilar matrix}
library(pheatmap)

df <- scale(banknote_num)

distanze <- dist(df, method = "euclidean")

diss_matrix <- as.matrix(distanze)

pheatmap(diss_matrix,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         show_rownames = FALSE,
         show_colnames = FALSE,
         color = colorRampPalette(c("blue", "white", "red"))(50),
         border_color = NA)
```

Applichiamo il clustering con metodo di linkage 'ward' che generalmente insieme all'average linkage, performa meglio.

```{r hierarchical agglomerative clustering hclust}
hclust_clustering <- hclust(distanze, method = "ward.D2")
```

Guardiamo il dendogramma cercando di identificare il numero ottimale di cluster, utilizziamo anche altri metodi che ci premetteranno di prendere una scelta migliore.

#### Numero di cluster
```{r hierarchical agglomerative clustering dend}
fviz_dend(hclust_clustering, show_labels = FALSE, palette = "jco", as.ggplot = TRUE)
```
Vediamo mediante il grafico Elbow 
```{r hierarchical agglomerative clustering elbow}
fviz_nbclust(df, FUN = hcut, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2) +
    labs(subtitle = "Elbow method")
```
Vediamo il grafico Silhouette 
```{r hierarchical agglomerative clustering hclust silhouette}
fviz_nbclust(df, FUN = hcut, method = "silhouette") +
    labs(subtitle = "Silhouette method")
```

Calcoliamo la Gap Statistics
```{r hierarchical agglomerative clustering hclust gap statistics}
set.seed(123)
fviz_nbclust(df, FUN = hcut, method = "gap_stat", nboot = 500)+
    labs(subtitle = "Gap statistic method")
```


```{r hierarchical agglomerative clustering hclust NbClust}
library("NbClust")

nb <- NbClust(df, distance = "euclidean", min.nc = 2, max.nc= 10, method = "ward.D2")
```

Generalmente si potrebbe optare per una scelta di "maggioranza" tra i vari metodi, tuttavia in questo caso hanno restituito tre risultati diversi. Si opta per considerare un numero di cluster pari a 3

#### Taglio del dendrogramma
```{r hierarchical agglomerative clustering hclust cut}
cluster_cut <- cutree(hclust_clustering, k = 3)

table(cluster_cut)

fviz_dend(hclust_clustering, k = 3,
        cex = 0.5,
        k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
        color_labels_by_k = TRUE,
        rect = TRUE
)
```

```{r hierarchical agglomerative clustering hclust pairs}
pairs(df, gap=0, pch=cluster_cut, col=c("orange", "green", "blue", "violet")[cluster_cut])
```

### Valutazione

Valutiamo ora il clustering ottenuto mediante tre metriche: silhouette, dunn index e Adjusted rand index.
```{r hierarchical agglomerative clustering hclust cut sil}
library(cluster)

sil <- silhouette(cluster_cut, distanze)

fviz_silhouette(sil, palette = "jco", ggtheme = theme_classic())
```

```{r hierarchical agglomerative clustering hclust dunn}
library(clusterCrit)

dunn_index <- intCriteria(as.matrix(df), cluster_cut, "Dunn")$dunn

print(paste("Dunn Index:", dunn_index))
```
```{r hierarchical agglomerative clustering hclust cri}
library("fpc")

status <- as.numeric(banknote$Status)

clust_stats <- cluster.stats(d = dist(df), status, cluster_cut)

print(paste("Corrected Rand Index:", clust_stats$corrected.rand))

print(paste("Meila's Index:", clust_stats$vi))
```
L'accordo tra i tipi di status e la soluzione di clustering è circa 0.792, utilizzando il corrected Rand index.
Il disaccordo tra i tipi di status e la soluzione di clustering è circa 0.359, utilizzando l'indice VI di Meila.
Entrambe le metriche suggeriscono che la soluzione di clustering è vicina alla vera suddivisione basata sullo status. La buona concordanza (alta CRI e basso VI) evidenzia che il metodo di clustering adottato è adeguato per distinguere correttamente i due gruppi (genuine e counterfeit) presenti nel dataset.

### Ottimizzazione

Eseguiamo ora un'ottimizzazione del clustering, trovando il numero di cluster e il metodo di linkage che ottimizzando le tre metriche viste prima.

```{r hierarchical agglomerative clustering linkage methods}
library(dplyr)

linkage_methods <- c("ward.D2", "single", "complete", "average", "centroid")

results_all <- data.frame(Method = character(),
                          k = numeric(),
                          Silhouette = numeric(),
                          Dunn = numeric(),
                          Corrected_Rand = numeric(),
                          VI = numeric(),
                          stringsAsFactors = FALSE)

min_k <- 2
max_k <- 10

for(method in linkage_methods) {
  hclust_obj <- hclust(distanze, method = method)
  
  for(k in min_k:max_k) {
    cluster_cut <- cutree(hclust_obj, k = k)
    
    sil <- silhouette(cluster_cut, distanze)
    avg_sil <- mean(sil[, 3])
    
    dunn_index <- intCriteria(as.matrix(df), cluster_cut, "Dunn")$dunn
    
    clust_stats <- cluster.stats(dist(df), status, cluster_cut)

    corrected_rand <- clust_stats$corrected.rand

    vi_val <- clust_stats$vi
    
    results_all <- rbind(results_all, data.frame(Method = method,
                                                 k = k,
                                                 Silhouette = avg_sil,
                                                 Dunn = dunn_index,
                                                 Corrected_Rand = corrected_rand,
                                                 VI = vi_val,
                                                 stringsAsFactors = FALSE))
  }
}

best_silhouette <- results_all %>% group_by(Method) %>% filter(Silhouette == max(Silhouette))
print(best_silhouette)

best_dunn <- results_all %>% group_by(Method) %>% filter(Dunn == max(Dunn))
print(best_dunn)

best_corrected_rand <- results_all %>% group_by(Method) %>% filter(Corrected_Rand == max(Corrected_Rand))
print(best_corrected_rand)

best_vi <- results_all %>% group_by(Method) %>% filter(VI == min(VI))
print(best_vi)
```
```{r hierarchical agglomerative clustering best}
library(dplyr)

results_ranked <- results_all %>%
  mutate(
    rank_sil = rank(-Silhouette, ties.method = "min"),
    rank_dunn = rank(-Dunn, ties.method = "min"),
    rank_corrected_rand = rank(-Corrected_Rand, ties.method = "min"),
    
    rank_vi = rank(VI, ties.method = "min"),
    
    total_rank = rank_sil + rank_dunn + rank_corrected_rand + rank_vi
  )

best_overall <- results_ranked %>% arrange(total_rank) %>% head(1)
print(best_overall)
```
Il clustering migliore con hclust sembrerebbe essere quello con metodo di linkage 'ward.D2' e numero di cluster pari a 2.

```{r hierarchical agglomerative clustering eclust}
final <- eclust(df, "hclust", k = 2, hc_metric = "euclidean", hc_method = "ward.D2", graph = FALSE)

fviz_cluster(final, geom = "point", ellipse.type = "norm", palette="jco", ggtheme = theme_minimal())
```

# ```{r hierarchical agglomerative clustering agnes}
# library("cluster")

# res.agnes <- agnes(x = banknote_num,
#                    stand = TRUE,
#                    metric = "euclidean",
#                    method = "ward"
# )

# fviz_dend(res.agnes, cex = 0.6, k = 3)

# ```

## 3.2 Clustering Gerarchico Divisivo

```{r hierarchical divisive clustering diana}
res.diana <- diana(x = banknote_num,
                   stand = TRUE,
                   metric = "euclidean"
)

fviz_dend(res.diana, cex = 0.6)
```

### Interpretazione del Dendrogramma

Il dendrogramma risultante mostra come i punti dati vengono uniti in cluster successivamente. L'altezza dei rami indica la distanza a cui avviene la fusione: rami più bassi corrispondono a fusioni tra punti o cluster molto simili. Analizzando il dendrogramma, possiamo decidere il
numero di cluster tagliando l'albero a un'altezza che separa i gruppi in modo significativo. Vantaggi e Svantaggi del Clustering Gerarchico

#### Vantaggi:

```
Non richiede la specifica del numero di cluster a priori.

Produce una rappresentazione gerarchica che può essere utile per comprendere le relazioni tra i dati.

Adatto a dataset di piccole e medie dimensioni.
```

#### Svantaggi:

```
Sensibile alla scelta della misura di distanza e del metodo di linkage.

Può essere computazionalmente costoso per dataset di grandi dimensioni. (quello divisivo)

Una volta effettuata una fusione, non è possibile separare i cluster, rendendo l'algoritmo meno flessibile in caso di errori.
```

# 3.2 Clustering Partizionale: K-means e K-medoids (PAM)

## 3.2.1 K-Means

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-means, dopo aver scalato le variabili numeriche del dataset.

```{r}
library(factoextra)
df <- scale(banknote[2:7])
```

### 3.2.1.1 Esecuzione dell'algoritmo con tutte le variabili selezionate

Successivamente si determina il numero ottimale di cluster da utilizzare nell'algoritmo, impiegando il metodo Elbow.

```{r}
fviz_nbclust(df, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
```

E il metodo Silhouette.

```{r}
fviz_nbclust(df, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

Il primo metodo indica quattro come numero ottimale di cluster, mentre il secondo ne suggerisce due. Poiché il dataset è composto da 200 osservazioni, equamente suddivise tra banconote autentiche e contraffatte, si sceglie di impostare il numero di cluster a due e si procede con l'esecuzione dell'algoritmo.

Per prima cosa si osserva una matrice di grafici a dispersione, utile per analizzare le relazioni tra le variabili numeriche del dataset.

```{r}
set.seed(123)
km.res <- kmeans(df, 2, nstart = 25)
print(km.res)

aggregate(banknote, by=list(cluster=km.res$cluster), mean)

dd <- cbind(banknote, cluster = km.res$cluster)
head(dd)

km.res$cluster

km.res$size

km.res$centers

cl <- km.res$cluster
pairs(df, gap=0, pch=cl, col=c("red", "blue")[cl])
```

TODO: commentare

Ora si analizza la distribuzione dei due cluster.

```{r}
fviz_cluster(km.res, 
             data = df,
             palette = c("red", "blue"),
             ellipse.type = "euclid", 
             star.plot = TRUE,
             repel = TRUE,
             ggtheme = theme_minimal()
)
```

### 3.2.1.2 Esecuzione dell'algoritmo con solamente le variabili bottom e diagonal

### 3.2.1.3 Esecuzione dell'algoritmo con solamente le variabili scelte tramite la PCA

## 3.2.2 K-Medoids 

### 3.2.2.1 Esecuzione dell'algoritmo sul dataset selezionando tutte le variabili

In questo capitolo i cluster vengono creati utilizzando l'algoritmo k-medoids con k impostato a 2.

Come in precedenza, si analizzano prima i grafici a dispersione e successivamente i due cluster.

```{r}
head(df, n = 3)

pam.res <- pam(df, 2)
print(pam.res)

dd <- cbind(banknote, cluster = pam.res$cluster)
head(dd, n = 8)

pam.res$medoids

head(pam.res$clustering)

cl <- pam.res$clustering
pairs(df, gap=0, pch=cl, col=c("#00AFBB", "#FC4E07")[cl])
```

Come previsto, i cluster risultano molto simili.

```{r}
fviz_cluster(pam.res,
             palette = c("#00AFBB", "#FC4E07"),
             ellipse.type = "t",
             repel = TRUE,
             ggtheme = theme_classic()
)
```

### 3.2.2.2 Esecuzione dell'algoritmo con solamente le variabili bottom e diagonal

### 3.2.2.3 Esecuzione dell'algoritmo con solamente le variabili scelte tramite la PCA

## 3.1.4 Misture di Gaussiane

In questa sezione introduciamo l'applicazione del clustering basato su misture di gaussiane, utilizzando la funzione `Mclust` del pacchetto **mclust**. Questo approccio assume che i dati siano generati da una combinazione di distribuzioni gaussiane e, tramite l'algoritmo EM (Expectation-Maximization), stima i parametri di ciascuna componente. Il vantaggio principale di questo metodo è la capacità di modellare cluster con forme ellissoidali, permettendo di gestire anche cluster con sovrapposizioni. Inoltre, `Mclust` effettua una selezione automatica del numero ottimale di cluster e del modello di covarianza (parsimonioso) in base al criterio BIC, fornendo un'indicazione della bontà dell'adattamento e della complessità del modello.  


```{r}
library(mclust)
data(banknote)
dati <- banknote[, -1]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
Nonostante sappiamo che il dataset banknote presenta due classi reali (ad esempio, banconote fraudolente e non fraudolente), l'analisi con Mclust ha individuato come miglior modello un numero di cluster pari a 3. Inoltre, il modello selezionato è di tipo VEE (ovvero ellissoidale, con forma variabile, ma con volume ed orientamento uguali), in base al valore più alto di BIC evidenziato nel plot. È interessante notare come il BIC per 2 cluster risulti significativamente inferiore rispetto a quello per un numero maggiore di cluster, probabilmente perché sono state utilizzate tutte le variabili del dataset per costruire i cluster.  
Confrontiamo esplicitamente i valori di BIC e ICL per soluzioni con 2 e 3 cluster:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, -1]
dati_scaled <- scale(dati)
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```
Dai sommari ottenuti si osserva che entrambi i modelli, per k = 2 e k = 3, forniscono valori che suggeriscono una soluzione migliore per 3 cluster. I valori del BIC e, conseguentemente, anche quelli dell'ICL (che include una penalizzazione per l'incertezza nelle assegnazioni), indicano che la soluzione a 3 cluster è preferibile rispetto a quella a 2 cluster.  
Valutiamo l'effetto della scelta delle variabili sul modello. Utilizziamo invece solo le due variabili "Bottom" e "Diagonal", individuate in precedenza dagli scatterplot come le più discriminanti:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, c("Bottom", "Diagonal")]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
Anche in questo caso si nota che il valore di BIC è notevolmente più alto (migliore) rispetto al caso in cui sono state usate tutte le variabili. L'algoritmo suggerisce ancora un numero ottimale di cluster pari a 3, ma questa volta ha selezionato come modello parsimonioso il modello EVE (ellissoidale, con volume uguale e orientamento variabile). È importante osservare che in uno dei cluster sono presenti solamente 16 osservazioni, il che potrebbe indicare un gruppo di dati che, probabilmente, sono state erroneamente assegnate a un cluster separato.  
Verifichiamo ora se i valori di ICL sono in accordo con quelli del BIC confrontando esplicitamente le soluzioni per 2 e 3 cluster:  

```{r}
library(mclust)
data(banknote)
dati <- banknote[, c("Bottom", "Diagonal")]
dati_scaled <- scale(dati)
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```
In questo caso, i valori di BIC e ICL sono concordi: per un numero di cluster pari a 3 si ottiene un BIC di circa -845 e un ICL di -849, mentre per 2 cluster il BIC risulta attorno a -888 e l'ICL a -890. Questo dimostra che anche l'ICL, che penalizza ulteriormente le assegnazioni incerte, suggerisce che il modello ottimale è quello con 3 cluster.  
Adesso, procederemo ad utilizzare solamente le variabili estratte tramite la tecnica PCA.  


```{r}
dati <- banknote_pca[, -1]
dati_scaled <- scale(dati)
gmm_result <- Mclust(dati_scaled)
summary(gmm_result)
plot(gmm_result, what = "BIC")
plot(gmm_result, what = "classification")
plot(gmm_result, what = "uncertainty")
plot(gmm_result, what = "density")
```
In questo caso si nota che il valore di BIC è più alto (migliore) rispetto al caso in cui sono state usate tutte le variabili ma è più basso del caso in cui sono state usate solamente le due variabili selezionate. Questa volta l'algoritmo suggerisce un numero ottimale di cluster pari a 2, selezionando come modello parsimonioso il modello EEV (ellissoidale, con volume e orientamento uguale).  
Verifichiamo ora se i valori di ICL sono in accordo con quelli del BIC confrontando esplicitamente le soluzioni per 2 e 3 cluster:  

```{r}
gmm_result2 <- Mclust(dati_scaled, G=2)
summary(gmm_result2)
gmm_result3 <- Mclust(dati_scaled, G=3)
summary(gmm_result3)
```

Anche in questo caso, i valori di BIC e ICL sono concordi: per un numero di cluster pari a 3 si ottiene un BIC di circa -1630 e un ICL di -1657, mentre per 2 cluster il BIC risulta attorno a -1614 e l'ICL a -1616. Questo dimostra che anche l'ICL suggerisce che il modello ottimale è quello con 2 cluster.  


## 3.1.5. Validazione e visualizzazione dei risultati

TODO: inserire commento introduttivo

### 3.1.5.3. Validazione con la classe reale

In questa sezione, esamineremo i diversi metodi di cluster utilizzati sul dataset 'banknote' fissando il numero di cluster pari al numero di classi reali, ossia pari a 2. In questo modo faremo uso di tecniche di validazione esterne perchè abbiamo a disposizione la reale classe delle osservazioni (variabile 'Status').  
Si farà uso di misure quali: adjusted Rand Index, Meila variation index e tutte le misure che hanno a che fare con il learning di classificazione (accuratezza, Specifity, Recall, Confusion Matrix, AUC, ecc.).  
Inizialmente tale misure verranno calcolate sull'intero dataset in cui sono stati eseguiti gli algoritmi per il calcolo dei cluster; successivamente sarà diviso il dataset in training e testing, nel primo saranno trovati i cluster con i diversi algoritmi e nel secondo saranno valutate le performance con tali misure, per evidenziare come si comportano questi ultimi per quanto riguarda la predizione di nuovi dati.

In questa sezione analizzeremo la qualità dei diversi metodi di clustering applicati al dataset banknote, fissando il numero di cluster pari al numero di classi reali, ovvero 2. Questo approccio ci permette di utilizzare tecniche di validazione esterne, dato che disponiamo delle etichette reali delle osservazioni (variabile 'Status').  
Per valutare l'efficacia dei cluster identificati, utilizzeremo diverse metriche, tra cui:  
- Adjusted Rand Index (ARI) e Meila Variation Index, che misurano il grado di concordanza tra il clustering ottenuto e la classificazione reale.
- Metriche di valutazione proprie del machine learning supervisionato, come accuratezza, specificità, recall, matrice di confusione e AUC, per interpretare il clustering come un problema di classificazione binaria.  

L'analisi verrà condotta in due fasi:  
1. Valutazione sull'intero dataset, per confrontare le prestazioni dei vari algoritmi di clustering utilizzando l'intera popolazione di dati.
2. Validazione su dati di test, dove il dataset verrà suddiviso in un set di training e un set di test. I cluster verranno individuati nel training set con i diversi algoritmi e poi utilizzati per classificare le osservazioni nel test set. Questo permetterà di valutare la capacità predittiva dei modelli nel riconoscere nuove osservazioni e analizzare la loro generalizzazione su dati non visti.


# ```{r}
# # Learning

# N=nrow(banknote)
# totIndex=seq(from=1,to=N,by=1)
# length(totIndex)
# gamma=0.7
# trainIndex=sample(totIndex,gamma*N, replace=F )
# banknote_L<-banknote[trainIndex,]
# nL=nrow(banknote_L)
# banknote_T<-banknote[-trainIndex,]
# nT=nrow(banknote_T)
# nL
# nT
# table(banknote$Status)
# table(banknote_L$Status)
# table(banknote_T$Status)
# #
# table(banknote$Status)/N*100
# table(banknote_L$Status)/nL*100
# table(banknote_T$Status)/nT*100


# #
# # what is the problem?
# #
# #install.packages("caret")
# library(caret) # CreateDataPartition is in the library "caret"
# # http://topepo.github.io/caret/data-splitting.html
# #
# #
# # Training, Validation and Test sets
# #
# gamma_L= 0.7 # proportion of units in the training set
# set.seed(1500)
# trainIndex <- createDataPartition(banknote$Status, p = gamma_L, 
#                                   list = FALSE, 
#                                   times = 1)
# banknote_L<-banknote[trainIndex,]
# nL<-nrow(banknote_L)
# nL
# table(banknote_L$Status)
# table(banknote_L$Status)/nL*100

# banknote_T<-banknote[-trainIndex,]
# nT<-nrow(banknote_T)
# nT
# table(banknote_T$Status)
# table(banknote_T$Status)/nT*100 


# set.seed(10000)
# bankmixt2_L<-mvnormalmixEM(banknote_L[,c(6,7)], k = 2)
# summary(bankmixt2_L)
# plot(bankmixt2_L, density = TRUE,  cex.axis = 1.4, cex.lab = 1.5, cex.main = 1.5)

# bankmixt2_L$class<-apply(bankmixt2_L$posterior,1,which.max)

# table(banknote_L$Status, bankmixt2_L$class)


# alpha=bankmixt2_L$lambda
# mu1=bankmixt2_L$mu[[1]]
# mu2=bankmixt2_L$mu[[2]]
# Sigma1=bankmixt2_L$sigma[[1]]
# Sigma2=bankmixt2_L$sigma[[2]]
# alpha
# mu1
# Sigma1
# mu2
# Sigma2

# library(mvtnorm)
# mu=rbind(mu1,mu2)
# mu
# p=2
# G=2
# Sigma=array(0, dim=c(p,p, G))
# Sigma[,,1]=Sigma1
# Sigma[,,2]=Sigma2
# Sigma

# stat_T_mixt=matrix(0,nT,1+2*G+1)
# source("mixt2_pg.r")
# for (i in 1:nT) {
#   x=banknote_T[i,c(6:7)]
#   stat_T_mixt[i,]=mixt2_pg(x,alpha,mu, Sigma,G)
# }
# table(stat_T_mixt[,6],banknote_T$Status)
# ```



